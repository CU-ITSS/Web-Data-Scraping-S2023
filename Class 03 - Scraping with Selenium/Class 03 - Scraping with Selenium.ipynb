{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2021 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping an API with `requests` and `json`, Wikipedia and Reddit\n",
    "* **Week 5**: Scraping data from Twitter\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "## Class 3 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Using Selenium to interact with websites\n",
    "* Implementing a screen-scraper with Selenium \n",
    "* Ethics of spoofing headers, screen scraping, and parallelizing API requests\n",
    "* Using Internet Archive API to find historical web pages\n",
    "* Retrieving and parsing Internet Archive pages\n",
    "\n",
    "Start with our usual suspect packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to servers on the web\n",
    "import requests\n",
    "\n",
    "# Parsing HTML magic\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Will be helful for converting between timestamps\n",
    "from datetime import datetime\n",
    "\n",
    "# We want to sleep from time-to-time to avoid overwhelming another server\n",
    "import time\n",
    "\n",
    "# We'll need to parse some strings, so we'll write some regular expressions\n",
    "import re\n",
    "\n",
    "from urllib.parse import quote, unquote\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below will only work once you've installed Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our interface to a real-life web browser... won't import until you install!\n",
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Selenium\n",
    "\n",
    "This is a non-trivial process: you will need to (1) install the Python bindings for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver. Follow the installation instructions in the documentation [here](https://selenium-python.readthedocs.io/installation.html) (you won't need the Selenium server).\n",
    "\n",
    "1. Install the Python bindings for Selenium. Go to your Anaconda terminal window, type in this command, and agree to whatever the package manager wants to install or update.\n",
    "\n",
    "`conda install selenium`\n",
    "\n",
    "2. Download the driver(s) for the web browser you want to use from the [links on the Selenium documentation](https://selenium-python.readthedocs.io/installation.html). If you use a Chrome browser, download the Chrome driver. Note that the Safari driver will not work on PCs and the Edge driver will not work on Macs. \n",
    "\n",
    "3. You will need to unzip the file and move the executable to the same directory where you are running this notebook. Make a note of the path to this directory.\n",
    "\n",
    "### Using Selenium to control a web browser\n",
    "The `driver` object we create is a connection from this Python environment out to the browser window.\n",
    "\n",
    "If you're on a Mac, the latest versions of OS X *really* do not like letting you run applications you've just downloaded. You'll need to dive into your system settings to fix it: https://support.apple.com/en-us/HT202491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome driver for my PC -- yours is likely very different\n",
    "# pc_path = 'E:/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver.exe'\n",
    "# driver = selenium.webdriver.Chrome(executable_path=pc_path)\n",
    "\n",
    "# Path to the Chrome driver for my Mac -- yours is likely very different\n",
    "mac_path = '/Users/briankeegan/Documents/GitHub/Web-Data-Scraping-S2021/Class 03 - Scraping with Selenium/geckodriver'\n",
    "driver = selenium.webdriver.Firefox(executable_path=mac_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single line of code will open a new browser window and will request the \"xkcd\" homepage.\n",
    "\n",
    "Your computer's security protocols may vigorously protest because you are launching a program that is controlled by another process/program. You will need to dismiss these warnings in order to proceed. Whether and how to do that will vary considerably across PCs and Macs, the kinds of permissions your account has on this operating system, and other security measures employed by your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://xkcd.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can use Python to control a web browser, there are a host of powerful functions you can use to simulate keystrokes, locate elements on a page, manage waits, *etc*.: https://selenium-python.readthedocs.io/\n",
    "\n",
    "In Classes 01 and 02, we used `BeautifulSoup` to turn HTML and XML into a data structure that we could search and access using Python-like syntax. With Selenium we use a standard called \"XPath\" to navigate through an HTML document: [this is the official tutorial](https://www.w3schools.com/xml/xpath_syntax.asp) for working with XPath. The syntax is different, but the intuition is similar: we can find a parent node by its attribute (class, id, *etc*.) and then navigate down the tree to its children.\n",
    "\n",
    "The XPath below has the following elements in sequence\n",
    "* `//` — Select all nodes that match the selection\n",
    "* `[@id=\"middleContainer\"]` — find the element that has a \"middleContainer\" id.\n",
    "* `/ul[2]` — select the second `<ul>` element underneath the `<div id=\"middleContainer\">`\n",
    "* `/li[3]` — select the third `<li>` element \n",
    "* `/a` — select the a element\n",
    "\n",
    "The combined XPath string `//*[@id=\"middleContainer\"]/ul[1]/li[3]/a` is like a \"file directory\" that (hopefully!) points to the hyperlink button that takes us to a random xkcd comic. With the directions to this button, we can have the web browser \"click\" the \"Random\" button beneath the comic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "\n",
    "# Once we've found it, now click it\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the attributes of different parts of the web page. xkcd is famous for its \"hidden messages\" inside the image alt-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "alttext_element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write a simple loop to click on the random button five times and print the alt-text from each of those pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(5):\n",
    "    time.sleep(1)\n",
    "    random_element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "    random_element.click()\n",
    "    \n",
    "    alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "    print('\\n',alttext_element.get_attribute(\"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done playing with your programmable web browser, make sure to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the connection to the web browser closed, any of the functions like `find_element_by_xpath`, `click()`, *etc*. will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "alttext_element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just about any operation you do in a web browser can be automated with Selenium: scrolling, clicking, completing forms, moving between tabs/windows, handling pop-ups, navigating back and forward, handling cookies, *etc*. Learn more about the functionality with tutorials and other resources in the [Selenium documentation](https://selenium-python.readthedocs.io/navigating.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Start your driver again and get the xkcd homepage.\n",
    "\n",
    "1. Change the XPath to click on the \"Prev\" button above the comic.\n",
    "2. Change the XPath to search for the \"comicNav\" class instead of the \"middleContainer\" id.\n",
    "3. Change the XPath to click on the \"About\" button in the upper-left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical web scraping\n",
    "\n",
    "James Densmore has a nice summary of [practices for ethical web scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
    "\n",
    "> * If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "> * I will always provide a User Agent string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "> * I will request data at a reasonable rate. I will strive to never be confused for a DDoS attack.\n",
    "> * I will only save the data I absolutely need from your page. If all I need it OpenGraph meta-data, that’s all I’ll keep.\n",
    "> * I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "> * I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "> * I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "> * I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
    "\n",
    "Some other important components of ethical web scraping practices [include](http://robertorocha.info/on-the-ethics-of-web-scraping/):\n",
    "\n",
    "* Reading the Terms of Service and Privacy Policies for the site's rules on scraping.\n",
    "* Inspecting the robots.txt file for rules about what pages can be scraped, indexed, *etc*.\n",
    "* Be gentle on smaller websites by running during off-peak hours and spacing out requests.\n",
    "* Identify yourself by name and email in your User-Agent strings\n",
    "\n",
    "What does a robots.txt file look like? Here is CNN's. It helpfull provides a sitemap to the robot to get other pages, it allows all kinds of User-agents, and disallows crawling of pages in specific directories (ads, polls, tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get('https://www.cnn.com/robots.txt').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are scraping websites, it is a good idea to include your contact information as a custom User-Agent string so that the webmaster can get in contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_header = {'User-Agent':'Python research tool by Brian Keegan, brian.keegan@colorado.edu'}\n",
    "\n",
    "request = requests.get('https://www.cnn.com',headers=contact_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adverse consequences of web scraping include:\n",
    "* Compromising the privacy and integrity of individual users' data\n",
    "* Damaging a web server with too many requests\n",
    "* Denying access to the web service to other authorized users\n",
    "* Infringing on copyrighted material\n",
    "* Damaging the business value of a web site\n",
    "\n",
    "[Amanda Bee](http://velociraptor.info/) compiled [a nice set of examples](https://github.com/amandabee/scraping-for-journalists/wiki/Reporting-Examples) of data journalists using web scraping for their reporting. There are some ethical justifications for violating a site's terms of service to scrape data:\n",
    "* Obtaining data for the public interest from official statements, government reports, *etc*.\n",
    "* Conducting audit studies (as long as these are responsibly designed and pre-cleared)\n",
    "* The data is unavailable from APIs, FOIA requests, and other reports\n",
    "\n",
    "[Sophie Chou](http://sophiechou.com/) made this nice [decision flow-chart](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/) of whether to build a scraper or not from a NICAR panel in 2016:\n",
    "\n",
    "![Should you build a scraper flowchart](http://www.storybench.org/wp-content/uploads/2016/04/flowchart_final.jpeg)\n",
    "\n",
    "Why is there a \"Talk to a lawyer?\" outcome at the bottom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Fraud and Abuse Act\n",
    "\n",
    "The [Computer Fraud and Abuse Act](https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act) was passed in 1984, [in large part due to](https://www.cnet.com/news/from-wargames-to-aaron-swartz-how-u-s-anti-hacking-law-went-astray/) the 1983 film [WarGames](https://en.wikipedia.org/wiki/WarGames) starring Matthew Broderick. A plain reading of the text of the law ([18 U.S.C. § 1030](https://www.law.cornell.edu/uscode/text/18/1030)) criminalizes just about any form of web scraping:\n",
    "\n",
    "> * Whoever intentionally accesses a computer without authorization or exceeds authorized access, and thereby obtains… information from any protected computer;\n",
    "> * knowingly causes the transmission of a program, information, code, or command, and as a result of such conduct, intentionally causes damage without authorization, to a protected computer;\n",
    "> * the term “exceeds authorized access” means to access a computer with authorization and to use such access to obtain or alter information in the computer that the accesser is not entitled so to obtain or alter;\n",
    "> * the term “damage” means any impairment to the integrity or availability of data, a program, a system, or information;\n",
    "> * the term “protected computer” means a computer which is used in or affecting interstate or foreign commerce or communication, including a computer located outside the United States that is used in a manner that affects interstate or foreign commerce or communication of the United States;\n",
    "\n",
    "Violators can be fined and jailed under a misdemeanor charge for up to 1 year for the first violation and jailed up to 10 years under a felony charge for repeated violations.\n",
    "\n",
    "This law has a [chilling effect](https://en.wikipedia.org/wiki/Chilling_effect) on many forms of research, journalism, and other forms of protected speech. The CFAA has been used by federal prosecutors to bring federal felony charges against programmers, journalists, and activists. In 2011, programmer and hacktivist [Aaron Swartz](https://en.wikipedia.org/wiki/Aaron_Swartz) (who contributed to the development of RSS, Markdown, Creative Commons, and Reddit) was [arrested and charged](https://en.wikipedia.org/wiki/United_States_v._Swartz) with violating the CFAA for downloading several million PDFs from JSTOR over MIT's network. The [decision to prosecute was unusual](https://www.huffingtonpost.com/2013/03/13/aaron-swartz-prosecutorial-misconduct_n_2867529.html). Facing 35 years of imprisonment and over $1 million in fines under the CFAA, Swartz committed suicide on January 11, 2013.\n",
    "\n",
    "In 2016, four computer science researchers and the publisher of *The Intercept* who all use scraping techniques to run experiments to measure bias and discrimination in web content [filed suit with the ACLU](https://www.aclu.org/cases/sandvig-v-sessions-challenge-cfaa-prohibition-uncovering-racial-discrimination-online) against the U.S. Government: *Sandvig v. Sessions*. Their research involves creating multiple fake accounts, providing inaccurate information to websites, using automated tools to record publicly-available data, and other scraping techniques. In March 2018, the [D.C. Circuit Court ruled](https://www.aclu.org/news/judge-allows-aclu-case-challenging-law-preventing-studies-big-data-discrimination-proceed) two of the plantiffs have standing to sue and the case is currently being prepared for trial.\n",
    "\n",
    "### Warning\n",
    "\n",
    "The code we will write and execute below will repeatedly violate YouTube's [Terms of Service](https://www.youtube.com/static?template=terms) (\"you are not allowed to... access the Service using any automated means (such as robots, botnets or scrapers)...\") for retrieving information from the platform. In effect, we will transmit code in excess of our authorized access and potentially cause damage, in order to obtain information from a protected computer. \n",
    "\n",
    "We will do this in order to obtain public statements made by goverment officials acting in their official capacity because this data is otherwise unavailable for retrieval from YouTube. There is an interesting body of emerging legal precedent treating elected officials' use of Twitter as a public forum: [*Knight First Amendment Institute v. Trump*](https://en.wikipedia.org/wiki/Knight_First_Amendment_Institute_v._Trump) established that [the President may not block other Twitter users](https://www.courtlistener.com/docket/6087955/72/knight-first-amendment-institute-at-columbia-university-v-trump/):\n",
    "\n",
    "> * \"We hold that portions of the @realDonaldTrump account -- the “interactive space” where Twitter users may directly engage with the content of the President’s tweets -- are properly analyzed under the “public forum” doctrines set forth by the Supreme Court, that such space is a designated public forum...\"\n",
    "> * \"we nonetheless conclude that the extent to which the President and Scavino can, and do, exercise control over aspects of the @realDonaldTrump account are sufficient to establish the government-control element as to the content of the tweets sent by the @realDonaldTrump account, the timeline compiling those tweets, and the interactive space associated with each of those tweets.\"\n",
    "> * \"Because a Twitter user lacks control over the comment thread beyond the control exercised over first-order replies through blocking, the comment threads -- as distinguished from the content of tweets sent by @realDonaldTrump, the @realDonaldTrump timeline, and the interactive space associated with each tweet -- do not meet the threshold criterion for being a forum.\"\n",
    "> * \"the account’s timeline, which “displays all tweets generated by the [account]”... all of which is government speech.\"\n",
    "\n",
    "On this basis, I believe the White House's videos posted to YouTube are government speech and our automated retrieval of this content and associated meta-data in violation of YouTube's Terms of Serice is justifiable for understanding this speech as a public forum.\n",
    "\n",
    "I would advise you against using these tools and approaches without a similarly clear public interest rationale and jurisprudence linking behavior to public forum doctrines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Screen-scraping Twitter with Selenium\n",
    "\n",
    "I am adapting a [tutorial by Shawn Wang](https://dev.to/swyx/scraping-my-twitter-social-graph-with-python-and-selenium--hn8) on scraping a Twitter graph with Python and Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome driver for my PC -- yours is likely very different\n",
    "# driver = selenium.webdriver.Chrome(executable_path='E:/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver.exe')\n",
    "\n",
    "# Path to the Chrome driver for my Mac -- yours is likely very different\n",
    "driver = selenium.webdriver.Firefox(executable_path=mac_path)\n",
    "\n",
    "driver.get('https://www.twitter.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually log in to your Twitter account through the driver page.\n",
    "\n",
    "Then go to the \"followings\" (or followees, also called \"friends\" in the Twitter API) of an account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/JoeBiden/following')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this Notebook's writing, the \"JoeBiden\" account followed 47 other accounts. Depending on the resolution of your display, size of the window, *etc*. there may only be 10–20 accounts visible. We can scroll to see the rest of these accounts programatically.\n",
    "\n",
    "Run this cell a few times to keep scrolling to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the HTML of the web page in the browser back to Python and turn it into soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, since I last ran this class in 2019 Twitter changed how they design and populate their website. If we inspect the elements for the following, we can see how they obfuscate the elements to make them hard to scrape. \n",
    "\n",
    "*Back in the day*, they had nice div tags with \"data-item-type\"s called \"user\". No longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.find_all('div', attrs={'data-item-type':'user'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-scraping YouTube with Selenium\n",
    "\n",
    "Let's get data from YouTube instead, which appears to be better-behaved from a web scraper's perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/c/whitehouse/videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube like Twitter also loads additional videos on scroll, so let's re-use the code above to scroll until we can't to load as many videos as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/51345544\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    time.sleep(.1)\n",
    "    html.send_keys(Keys.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded as many videos as the YouTube interface allows by scrolling, pull the contents of the web page. We can revert back to our strategies from Week 2 to identify, navigate, and pull out the relevant fields we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the source, the video cells appear to live within elements called `<ytd-grid-video-renderer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs = soup.find_all('ytd-grid-video-renderer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further drill-down and inspection of this element from the browser reveals that some promising data lives within an element defined as `<a id=\"video-title>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('a',{'id':'video-title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the `aria-label` string there's the title, account, a relative date and a detailed number of views. Within the `href` tag is the video ID. These all seem like promising bits of data to try to grab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['aria-label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regular expressions to try to match the numeric fields like the number of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = video_divs[-1].find_all('a',{'id':'video-title'})[0]['aria-label']\n",
    "re.findall(r'([\\d,]+) views',_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract other relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The video link\n",
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The video title\n",
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also some helpful data about the length of the video hiding in a `<ytd-thumbnail-overlay-time-status-renderer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('ytd-thumbnail-overlay-time-status-renderer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out that video length element with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('ytd-thumbnail-overlay-time-status-renderer')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also labels about whether the videos are closed captioned within a tag called `<span class=\"style-scope ytd-badge-supported-renderer\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first video (at the bottom) has a CC tag, the second one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-2].find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these pieces together into a loop that grabs all the data from this list of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_l = []\n",
    "\n",
    "for d in video_divs:\n",
    "    # Get the number of views\n",
    "    _s = d.find_all('a',{'id':'video-title'})[0]['aria-label']\n",
    "    _views = re.findall(r'([\\d,]+) views',_s)[0]\n",
    "    \n",
    "    # Get the link\n",
    "    _link = d.find_all('a',{'id':'video-title'})[0]['href']\n",
    "    \n",
    "    # Get the title\n",
    "    _title = d.find_all('a',{'id':'video-title'})[0]['title']\n",
    "    \n",
    "    # Get the length\n",
    "    _length = d.find_all('ytd-thumbnail-overlay-time-status-renderer')[0].text.strip()\n",
    "    \n",
    "    # Get the captioning\n",
    "    if len(d.find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})) > 1:\n",
    "        _cc = True\n",
    "    else:\n",
    "        _cc = False\n",
    "        \n",
    "    # Package it all up into a dictionary\n",
    "    _d = {'Views':_views,\n",
    "          'Link':_link,\n",
    "          'Title':_title,\n",
    "          'Length':_length,\n",
    "          'Captioned':_cc\n",
    "         }\n",
    "    \n",
    "    # Add our dictionary to the container\n",
    "    videos_l.append(_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the `videos_l` container into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(videos_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your priorities, you could stop here and save this to a CSV since there is already rich data. \n",
    "\n",
    "Some limitations to think of include:\n",
    "* How would YouTube serve up an account with hundreds or thousands of videos? Is there a limit to the videos you can get from scrolling?\n",
    "* The \"Views\" columns are stored as strings not as numeric values: you'll want to convert them somehow. Those commas could also complicate things when storing as a *comma separated* file, so you'll want to strip them out too somehow. Fixing both of these are related.\n",
    "* We don't have any of the valuable data about the actual date the video was posted, the number of up/down votes, or even the transcript from the captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data from each video's page\n",
    "\n",
    "There's valuable data on each video's page about the specific date, the up and down votes, and even the transcript of the video that we can also retrieve.\n",
    "\n",
    "Let's start with the inauguration video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/watch?v=q5iCPKDp4V4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw markdown and soup-ify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "yt_inauguration_soup = BeautifulSoup(yt_inauguration_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date the video was uploaded appears within a `<div id=\"date\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging in, we can pull out the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'date'})[0].text[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up and downvotes appear as coarse aggregations (\"16K\",\"95K\") but the true counts at the time are hidden in some \"aria-label\"s. \n",
    "\n",
    "First, find the `<button id=\"button\">` within the `<div id=\"top-level-buttons\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'top-level-buttons'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's apparently (and hopefully only!) two of these types of divs. The one we care about is the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlb2 = yt_inauguration_soup.find_all('div',{'id':'top-level-buttons'})[1]\n",
    "\n",
    "tlb2.find_all('button',{'id':'button'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up and down votes are the first two buttons and include an `aria-label` with a count of the number of up and down votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes = tlb2.find_all('button',{'id':'button'})[0]\n",
    "downvotes = tlb2.find_all('button',{'id':'button'})[1]\n",
    "\n",
    "upvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the `aria-label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes['aria-label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a regex to extract the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([\\d,]+) other people',upvotes['aria-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([\\d,]+) other people',downvotes['aria-label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For closed captioned videos, there's also a transcript of the video with timestamps and text within the `<ytd-transcript-renderer>` parent or `<div class=\"cue-group\">` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'class':'cue-group'})[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `<div class=\"cue-group\">` elements, we can extract the time code and the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cg = yt_inauguration_soup.find_all('div',{'class':'cue-group'})[-1]\n",
    "last_cg.find_all('div',{'class':'cue-group-start-offset'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cg.find_all('div',{'class':'cue'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the whole transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_l = []\n",
    "\n",
    "for cg in yt_inauguration_soup.find_all('div',{'class':'cue-group'}):\n",
    "    _time_code = cg.find_all('div',{'class':'cue-group-start-offset'})[0].text.strip()\n",
    "    _text = cg.find_all('div',{'class':'cue'})[0].text.strip()\n",
    "    \n",
    "    _d = {'Time':_time_code,\n",
    "          'Text':_text.replace('\\n',' ')}\n",
    "    \n",
    "    cg_l.append(_d)\n",
    "    \n",
    "pd.DataFrame(cg_l).set_index('Time')['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple videos\n",
    "That was all to extract the data from a single video. Let's now scrape the content from each of the White House YouTube videos. With something like 160 videos times 2 seconds per video, this scrape should take just over 5 minutes. So let's let this run and take a break. Something will likely break, so let's check back in afterwards! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos_l:\n",
    "    _link = v['Link']\n",
    "    \n",
    "    # Have Selenium get the page, get the source, and convert to soup\n",
    "    driver.get('https://www.youtube.com'+_link)\n",
    "    \n",
    "    # Give the page a second to load\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Retrieve the content and soup-ify\n",
    "    _raw = driver.page_source.encode('utf-8')\n",
    "    _soup = BeautifulSoup(_raw)\n",
    "    \n",
    "    # Get the date\n",
    "    try:\n",
    "        _date = _soup.find_all('div',{'id':'date'})[0].text[1:]\n",
    "    except:\n",
    "        # If we get an index error above, the page didn't finish loading\n",
    "        # Wait another 2 seconds\n",
    "        time.sleep(2)\n",
    "        _date = _soup.find_all('div',{'id':'date'})[0].text[1:]\n",
    "    \n",
    "    # Get the up and downvotes\n",
    "    try:\n",
    "        _tlb2 = _soup.find_all('div',{'id':'top-level-buttons'})[-1]\n",
    "        _upvotes_soup = _tlb2.find_all('button',{'id':'button'})[0]\n",
    "        _downvotes_soup = _tlb2.find_all('button',{'id':'button'})[1]\n",
    "        _upvotes = re.findall(r'([\\d,]+) other people',_upvotes_soup['aria-label'])[0]\n",
    "        _downvotes = re.findall(r'([\\d,]+) other people',_downvotes_soup['aria-label'])[0]\n",
    "    except:\n",
    "        # If we get an index error above, the page didn't finish loading\n",
    "        # Wait another 2 seconds\n",
    "        time.sleep(2)\n",
    "        _tlb2 = _soup.find_all('div',{'id':'top-level-buttons'})[-1]\n",
    "        _upvotes_soup = _tlb2.find_all('button',{'id':'button'})[0]\n",
    "        _downvotes_soup = _tlb2.find_all('button',{'id':'button'})[1]\n",
    "        _upvotes = re.findall(r'([\\d,]+) other people',_upvotes_soup['aria-label'])[0]\n",
    "        _downvotes = re.findall(r'([\\d,]+) other people',_downvotes_soup['aria-label'])[0]\n",
    "    \n",
    "    # Update the dictionary\n",
    "    v['Date'] = _date\n",
    "    v['Upvotes'] = _upvotes\n",
    "    v['Downvotes'] = _downvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the revised `videos_l` to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitehouse_yt_df = pd.DataFrame(videos_l)\n",
    "\n",
    "# Convert the Views, Upvotes, and Downvotes columns to ints\n",
    "whitehouse_yt_df['Views'] = whitehouse_yt_df['Views'].fillna('0').str.replace(',','').astype(int)\n",
    "whitehouse_yt_df['Upvotes'] = whitehouse_yt_df['Upvotes'].fillna('0').str.replace(',','').astype(int)\n",
    "whitehouse_yt_df['Downvotes'] = whitehouse_yt_df['Downvotes'].fillna('0').str.replace(',','').astype(int)\n",
    "\n",
    "# Convert the Date to a datetime\n",
    "whitehouse_yt_df['Date'] = pd.to_datetime(whitehouse_yt_df['Date'])\n",
    "\n",
    "whitehouse_yt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoofing headers\n",
    "\n",
    "When we use `requests` or Selenium to get data from other web servers, each of the get requests carries some meta-data about ourselves, called [headers](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html). These headers tell the server what kind of web browser we are, what kinds of data we can receive, *etc*. so that the server can reply with properly-formatted information. \n",
    "\n",
    "But it is also possible for the server to understand a request and refuse to fulfill it, known as a [HTTP 403 error](https://en.wikipedia.org/wiki/HTTP_403). A server's refusal to fulfill a client's request can often be traced back to the identity a client presents through its headers or a client lacking authorization to access the data (*i.e.*, you need to authenticate with the website first). In the case of `requests`, its `get` request includes default header information that identifies it as a Python script rather than a human-driven web browser.\n",
    "\n",
    "Let's make a request for an article from the NYTimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_response = requests.get('https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the headers we sent with this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the 'User-Agent' string identifies this request as originating from the \"python-requests/2.21.0\" program, rather than a typical web browser. Some web servers will be configured to inspect the headers of incoming requests and refuse requests unless they are actual web browsers.\n",
    "\n",
    "We can often circumvent these filters by sending alternative headers that claim to be from a web browser as a part of our `requests.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with spoofed headers for the User-Agent\n",
    "spoofed_headers = {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "\n",
    "# Make the request with the \n",
    "nytimes_url = 'https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html'\n",
    "spoofed_response = requests.get(nytimes_url,headers=spoofed_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the get request we sent to the NYTimes web server now includes the spoofed \"User-Agent\" string we wrote that claims our request is from a web browser. The server should now return the data we requested, even though we are not who we claimed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoofed_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble finding a website that refused \"python-requests\" connections automatically (*e.g.*, Amazon, NYTimes, etc.), but you will likely find some along the way. \n",
    "\n",
    "Spoofing headers to conceal the identity of your client to a web server is another example of how technological capabilities can overtake ethical responsibilities. The owners of a web server may have good reasons for refusing to serve content to non-web browsers (copyright, privacy, business model, *etc*.). Misrepresenting your identity to extract this data should only be done if the risks to others are small, the benefits are in the public interest, there are no other alternatives for obtaining the data, *etc*. \n",
    "\n",
    "There can be *very* real consequences for spoofing headers. Because it is such a common and relatively trivial method for circumventing server security settings, making repeated spoofed requests could result in your IP address or an IP address range (worst case, the entire university) being blocked from making requests to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing requests\n",
    "\n",
    "A third web scraping practice that warrants ethical scrutiny is parallelization. In the example of getting historical `@WhiteHouse` tweets, we launched a single browser window and \"scrolled\" until we reached the end; a process that took on the order of a minute.\n",
    "\n",
    "However, we *could* launch multiple scripts that each creates a browser windows and collect different segments of the data in parallel for us to combine the results at the end. In an API context, we *could* create multiple applications and design our requests so that each works simultaneously to get all the data. \n",
    "\n",
    "Each request imposes some cost on the server to receive, process, and return the requested data: making these requests in parallel increases the convenience and efficiency for the data scraper, but also dramatically increases the strain on the server to fulfill other clients' requests. In fact, highly-parallelized and synchronized requests can look like [denial-of-service attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack) and may get your requests far more scrutiny and blowback than patiently waiting for your data to arrive in series. The ethical justifications for employing highly-parallelized scraping approaches are thin: documenting a rapidly-unfolding event before the data disappears, for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
