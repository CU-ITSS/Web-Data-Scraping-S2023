{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2021 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping an API with `requests` and `json`, Wikipedia and Reddit\n",
    "* **Week 5**: Scraping data from Twitter\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "This notebook is adapted from excellent notebooks in Dr. [Cody Buntain](http://cody.bunta.in/)'s seminar on [Social Media and Crisis Informatics](http://cody.bunta.in/teaching/2018_winter_umd_inst728e/) as well as the [PRAW documentation](https://praw.readthedocs.io/en/latest/).\n",
    "\n",
    "## Class 4 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Accessing an API with `requests`\n",
    "* Retrieiving historical web data from the Internet Archive\n",
    "* Why you don't want to write a parser for Wikipedia's data\n",
    "* Fundamentals of retrieving information from Wikipedia's API\n",
    "* EDA with data from Wikipedia's API\n",
    "* Reddit's API, time permitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a few common libraries for all these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to other servers on the web\n",
    "import requests\n",
    "\n",
    "# APIs spit out data in JSON\n",
    "import json\n",
    "\n",
    "# Use BeautifulSoup to parse some HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Handling dates and times\n",
    "from datetime import datetime\n",
    "\n",
    "# DataFrames!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Internet Archive's Wayback Machine\n",
    "\n",
    "Now we'll leave some of the ethically-fraught methods of web scraping behind. The Internet Archive maintains the \"[Wayback Machine](https://www.archive.org/web/)\" where old versions of websites are stored. Some of my favorites:\n",
    "\n",
    "* [CNN in June 2000](https://web.archive.org/web/20000815052826/http://www.cnn.com/)\n",
    "* [Facebook in August 2004](https://web.archive.org/web/20040817020419/http://www.facebook.com/)\n",
    "* [Apple in April 1997](https://web.archive.org/web/19970404064444/http://www.apple.com:80/)\n",
    "\n",
    "In these URLs above, there is a numeric identifier corresponding to the timestamp when the image of the website was captured. How do we know when the Wayback Machine archived a webpage? There's a free and open API!\n",
    "\n",
    "### Using the Wayback Machine API\n",
    "\n",
    "The simplest API request we can make asks for the most recent snapshot of a webpage archived by the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response tells us the timestamp and location of this snapshot, which we could then go retrieve and parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_response_json = wb_response.json()\n",
    "\n",
    "recent_fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "recent_fb_wb_response = requests.get(recent_fb_wb_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw text out, soupify, and look for links. For some reason all the links in this snapshot are in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_fb_wb_raw = recent_fb_wb_response.text\n",
    "\n",
    "recent_fb_wb_soup = BeautifulSoup(recent_fb_wb_raw)\n",
    "\n",
    "[link.text for link in recent_fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for the most recent snapshot of a webpage around a specific date. Let's ask the Wayback Machine for a snapshot of Facebook around February 1, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com&timestamp=20080201'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response_json = wb_response.json()\n",
    "\n",
    "wb_response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a relatively deep JSON object we have to navigate into to access information like the Wayback URL or the timestamp of the snapshot. The closest snapshot to February 1, 2008 was January 30, 2008. We use the [`datetime.strptime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) function to turn this numeric string that we recognize as a timestamp into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_response_json['archived_snapshots']['closest']['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.strptime(wb_response_json['archived_snapshots']['closest']['timestamp'],\n",
    "                        '%Y%m%d%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we could scrape out the links on this 2008 version of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "fb_wb_response = requests.get(fb_wb_url)\n",
    "\n",
    "# Get the text from the response\n",
    "fb_wb_raw = fb_wb_response.text\n",
    "\n",
    "# Soup-ify\n",
    "fb_wb_soup = BeautifulSoup(fb_wb_raw)\n",
    "\n",
    "# Make a list of the text of the links\n",
    "[link.text for link in fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping historical web pages\n",
    "\n",
    "A current project I am working on is exploring how social media platforms' terms of service have evolved over time. Let's start with Facebook's terms of service and privacy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_tos = 'http://www.facebook.com/terms.php'\n",
    "fb_pp = 'http://www.facebook.com/policy.php'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take advantage of the [`date_range`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html) fuction in `pandas` to generate a range of dates between January 2005 and January 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = pd.date_range(start='2005-01-01',end='2021-03-01',freq='M')\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [`datetime.strftime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) (the inverse of `strptime`) to make these date objects into specifically-formatted strings that we can format into a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first datetime object and turn it into a string\n",
    "datetime.strftime(dates_list[0],'%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use string formatting to put the `fb_tos` URL and formatted timestamp into a request to the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.strftime(dates_list[0],'%Y%m%d')\n",
    "\n",
    "wb_api_url = 'https://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "wb_api_url_formatted = wb_api_url.format(fb_tos,date_str)\n",
    "\n",
    "print(wb_api_url_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the request to the Wayback Machine to get the URL and timestamp of the Wayback Machine's closest snapshot of Facebook's Terms of Service before January 31, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api_response = requests.get(wb_api_url_formatted)\n",
    "\n",
    "wb_api_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the markup of this old version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "wb_fb_old_url = wb_api_response.json()['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "wb_fb_raw = requests.get(wb_fb_old_url).text\n",
    "\n",
    "# Soup-ify\n",
    "wb_fb_soup = BeautifulSoup(wb_fb_raw)\n",
    "\n",
    "# Find the content element and get the text out\n",
    "wb_fb_terms_str = wb_fb_soup.find('div',{'id':'content'}).text.strip()\n",
    "\n",
    "# Inspect\n",
    "wb_fb_terms_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a really dumb stemmer, [`.split()`](https://docs.python.org/3.7/library/stdtypes.html#str.split) to count the number of words in these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wb_fb_terms_str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop to find a snapshot of Facebook's ToS each month in our `dates_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url_str,start_date='2005-01-01',end_date='2021-03-01',freq='M'):\n",
    "    \n",
    "    # Make the list of dates\n",
    "    date_l = pd.date_range(start_date,end_date,freq=freq)\n",
    "    \n",
    "    # Create an empty container to store our data\n",
    "    urls = dict()\n",
    "\n",
    "    # For each date in the list of dates\n",
    "    for date in date_l:\n",
    "        \n",
    "        # Turn the date object back into a string\n",
    "        date_str = datetime.strftime(date,'%Y%m%d%H%M%S')\n",
    "        \n",
    "        # Define the API URL request to the Wayback machine\n",
    "        wb_api_url = 'http://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "        \n",
    "        # Format the API URL with the URL of the website and the closest datetime\n",
    "        wb_api_request = wb_api_url.format(url_str,date_str)\n",
    "        \n",
    "        # Make the request\n",
    "        r = requests.get(wb_api_request).json()\n",
    "\n",
    "        # Check if the returned request has all the right parts (this is probably overkill)\n",
    "        if 'archived_snapshots' in r.keys():\n",
    "            if 'closest' in r['archived_snapshots'].keys():\n",
    "                if 'url' in r['archived_snapshots']['closest'].keys():\n",
    "                    \n",
    "                    # If it does have all the right parts, get the URL\n",
    "                    _url = r['archived_snapshots']['closest']['url']\n",
    "                    \n",
    "                    # Get the timestamp\n",
    "                    _timestamp = r['archived_snapshots']['closest']['timestamp']\n",
    "                    \n",
    "                    # Save to our URL dictionary with the timestamp of the snapshot as key, the url as value\n",
    "                    urls[_timestamp] = _url\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our function to make a dictionary of keys returning the Wayback Machine URLs for each month's version of the terms of service. We'll write a loop to get the Terms for each snapshot and count the words. \n",
    "\n",
    "This will take a few minutes. \n",
    "\n",
    "I've coverted the code block into a \"Raw\" cell to prevent accidental execution. You can always turn it into a \"Code\" cell if you really want to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of timestamps and URLs for each monthly version of the Terms of Service\n",
    "fb_terms_d = get_urls('https://www.facebook.com/terms.php')\n",
    "\n",
    "# Create an empty container to store our data\n",
    "fb_terms_wordcount = {}\n",
    "\n",
    "# Loop through the fb_terms_d dictionary\n",
    "for timestamp,url in fb_terms_d.items():\n",
    "\n",
    "    # Get the raw HTML from the Wayback Machine\n",
    "    raw = requests.get(url).text\n",
    "    \n",
    "    # Soup-ify\n",
    "    soup = BeautifulSoup(raw)\n",
    "    \n",
    "    # Find the content of the TOS\n",
    "    content = soup.find('div',{'id':'content'}).text.strip()\n",
    "    \n",
    "    # Split the content into words, count the number of words, save to the container\n",
    "    fb_terms_wordcount[timestamp] = len(content.split())\n",
    "    \n",
    "# Write to disk\n",
    "with open('facebook_tos_archive.json','w') as f:\n",
    "    json.dump(fb_terms_wordcount,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_tos_archive.json','w') as f:\n",
    "    json.dump(fb_terms_wordcount,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having everyone hit the Internet Archive server with the same requests, you can also load this file with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_tos_archive.json','r') as f:\n",
    "    fb_terms_wordcount2 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the changes in the size of Facebook's Terms of Service over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dictionary into a pandas Series\n",
    "fb_terms_s = pd.Series(fb_terms_wordcount2)\n",
    "\n",
    "# Conver the index to datetime objects\n",
    "fb_terms_s.index = pd.to_datetime(fb_terms_s.index)\n",
    "\n",
    "# Plot\n",
    "ax = fb_terms_s.plot()\n",
    "\n",
    "# Make the x-tick labels less weird\n",
    "# ax.set_xticklabels(range(2004,2021,2),rotation=0,horizontalalignment='center')\n",
    "\n",
    "# Always label your axes\n",
    "ax.set_ylabel('Word count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wikipedia\n",
    "\n",
    "Consider the Wikipedia page for [George H.W. Bush](https://en.wikipedia.org/wiki/George_H._W._Bush). This seems like a relatively straightforward webpage to scrape out the hyperlinks to other articles or to compare the content to other presidential biographies. However, Wikipedia also preserves the [history of every revision made to this article](https://en.wikipedia.org/w/index.php?title=George_H._W._Bush&action=history) going back to the first (available) revisions in 2001, like [this](https://en.wikipedia.org/w/index.php?title=George_H._W._Bush&oldid=345784898). Thinking back to the Oscars example, it seems promising to find the \"oldid\" values and visit each revision's webpage to parse the content out. However, Wikipedia will give you much of this revision history data for free through its [application programming interface](http://en.wikipedia.org/w/api.php) (API).\n",
    "\n",
    "### Current content\n",
    "We can use `requests` to get the current HTML markup of an article from the API, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the API server lives\n",
    "query_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# An empty dictionary to store our query parameters\n",
    "query_params = {}\n",
    "\n",
    "# We want to parse the content of a page\n",
    "query_params['action'] = 'parse'\n",
    "\n",
    "# Which page?\n",
    "query_params['page'] = 'George H. W. Bush'\n",
    "\n",
    "# We want the text\n",
    "query_params['prop'] = 'text'\n",
    "\n",
    "# Ignore the edit buttons and table of contents\n",
    "query_params['disableeditsection'] = 1\n",
    "query_params['disabletoc'] = 1\n",
    "\n",
    "# Get the results back as JSON\n",
    "query_params['format'] = 'json'\n",
    "\n",
    "# Format the data in an easier-to-parse option\n",
    "query_params['formatversion'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only set up our request to the API, but not sent it or received the data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = requests.get(url = query_url, params = query_params).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's waiting inside? A dictionary of dictionaries. The inner dictionary has keys for the title of the page we requested (\"George H. W. Bush\"), the pageid (a numeric identifier), and the text of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response['parse'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could count the number of links in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghwb_soup = BeautifulSoup(json_response['parse']['text'])\n",
    "\n",
    "ghwb_soup.find_all('a')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the content of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghwb_soup.find_all('p')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision history\n",
    "\n",
    "There is also an API endpoint for the revision history of this article that contains metadata about the who and when of previous changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the API server lives\n",
    "query_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# An empty dictionary to store our query parameters\n",
    "query_params = {}\n",
    "\n",
    "# We want to query properties of a page\n",
    "query_params['action'] = 'query'\n",
    "\n",
    "# Which page?\n",
    "query_params['titles'] = 'George H. W. Bush'\n",
    "\n",
    "# We want the revisions\n",
    "query_params['prop'] = 'revisions'\n",
    "\n",
    "# In particular, we want the revision ids, users, comments, timestamps\n",
    "query_params['rvprop'] = 'ids|userid|comment|timestamp|user|size|sha1'\n",
    "\n",
    "# Get 500 revisions\n",
    "query_params['rvlimit'] = 500\n",
    "\n",
    "# Start old and go newer\n",
    "query_params['rvdir'] = 'newer'\n",
    "    \n",
    "# Get the results back as JSON\n",
    "query_params['format'] = 'json'\n",
    "\n",
    "# Format the data in an easier-to-parse option\n",
    "query_params['formatversion'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = requests.get(url = query_url, params = query_params).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect this `json_response`. This returns a dictionary with both \"continue\" and \"query\" keys. The continue indicates there are more than 500 revisions present in the article's history and provides an index for the next query to pick up from. The query contains the revision history we care about—buried a bit in a nested data structure of lists and dictionaries, but we eventually get to the \"revisions\" list of dictionaries with the revision histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = json_response['query']['pages'][0]['revisions']\n",
    "revisions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = pd.DataFrame(revisions)\n",
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out how the size of the article changed over the first 500 revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rev_df.plot(y='size',legend=False)\n",
    "ax.set_ylabel('Size (bytes)')\n",
    "ax.set_xlabel('Revision')\n",
    "ax.set_xlim((0,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or count how many times an editor made a contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['user'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other parts of the very powerful Wikipedia API and scraping these APIs exposes much more metadata than parsing the HTML of these webpages, while also being easier on the servers hosting it. I will share a notebook that has functions for retrieving and parsing content, revisions, pageviews, and other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit\n",
    "\n",
    "Reddit also hosts a lot of detailed behavioral data that could be of interest to social scientists. As was the case with Wikipedia, our naïve inclination may be to develop scrapers and parsers to extract this information, but Reddit will give much of it to you for free through their API!\n",
    "\n",
    "You can retrieve a few different types of entities from Reddit's API: sub-reddits, submissions, comments, and redditors. Many of these are interoperable: a sub-reddit contains submissions contributed by redditors with comments from other redditors.\n",
    "\n",
    "We will use a wrapper library to communicate with the Reddit API called [Python Reddit API Wrapper](https://praw.readthedocs.io/en/latest/) or `praw`. \n",
    "\n",
    "Copy the code below to your terminal to install `praw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conda install -c conda-forge praw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can import `praw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to authenticate with Reddit to get access to the API. Typically you can just enter the client ID, client secret, password, username, *etc*. as strings. \n",
    "\n",
    "1. You will need to create an account on Reddit. After you have created an account and logged in, go to https://www.reddit.com/prefs/apps/. \n",
    "2. Scroll down and click the \"create app\" button at the bottom. Provide a basic name, description, and enter a URL for your homepage (or just use http://www.colorado.edu).\n",
    "3. You will need the client ID (the string of characters beneath the name of your app) as well as the secret (the other string of characters) as well as your username and password.\n",
    "4. You can make up a user-agent string, but include your username as good practice for the sysadmins to track you down if you break things.\n",
    "\n",
    "![Image from Cody Buntain](http://www.cs.umd.edu/~cbuntain/inst728e/reddit_screens/1-003a.png)\n",
    "\n",
    "You'll create an API connector object (`r`) below that will authenticate with the API and handle making the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = praw.Reddit(client_id='your application id',\n",
    "                client_secret='your application secret',\n",
    "                password='your account password',\n",
    "                user_agent='scraping script by /u/youraccountname',\n",
    "                username='your account name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that this authentication process worked by making a simple request like printing your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to read them in from a local file (\"login.json\") so that I post this notebook on the internet in the future without compromising my account security. This won't work for you, so just skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load my credentials from a local disk so I don't show the world\n",
    "with open('reddit_login.json','r') as f:\n",
    "    r_creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated reddit instance using the creds\n",
    "r = praw.Reddit(client_id = r_creds['client_id'],\n",
    "                client_secret = r_creds['client_secret'],\n",
    "                password = r_creds['password'],\n",
    "                user_agent = r_creds['user_agent'],\n",
    "                username = r_creds['username'])\n",
    "\n",
    "# Make sure your reddit instance works\n",
    "print(r.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-reddits\n",
    "Now print the top 25 stories in /r/news.\n",
    "\n",
    "[Documentation for the Subreddit model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `news_subreddit` object to store the various attributes about this sub-reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit = r.subreddit('news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `news_subreddit` has a number of attributes and methods you can call on it. The time the sub-reddit was founded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.created_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's formatted in a UNIX timecode (seconds since 1 January 1970), but we can convert it into a more readable timestamp with `datetime`'s `utcfromtimestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.utcfromtimestamp(news_subreddit.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other attributes such as the number of subscribers, current active users, as well as the description of the sub-reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{0:,}'.format(news_subreddit.subscribers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.over18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.active_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of the sub-reddit are available as a method `.rules()` which returns a list of dictionaries of rule objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.rules()['rules']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When were each of these rules created? Loop through each of the rules and print the \"short_name\" of the rule and the rule timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in news_subreddit.rules()['rules']:\n",
    "    created = rule['created_utc']\n",
    "    print(rule['short_name'], datetime.utcfromtimestamp(created))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a list of the moderators for this subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = []\n",
    "\n",
    "for mod in news_subreddit.moderator():\n",
    "    mod_list.append(mod.name)\n",
    "    \n",
    "mod_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "We can get a list of submissions to a sub-reddit using [a few different methods](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html).\n",
    "\n",
    "* `.controversial()`\n",
    "* `.hot()`\n",
    "* `.new()`\n",
    "* `.rising()`\n",
    "* `.search()`\n",
    "* `.top()`\n",
    "\n",
    "Here we will use the `.top()` method to get the top 25 submissions on the /r/news subreddit from the past 12 months.\n",
    "\n",
    "[Documentation for the Submission model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_news = r.subreddit('news').top('year',limit=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top25_news` is a `ListingGenerator` object, which is a special [generator](https://www.dataquest.io/blog/python-generators-tutorial/) class defined by PRAW. It does not actually go out and get the data at this stage. There's not much you can do to look inside this `ListingGenerator` other than loop through and perform operations. In this case, lets add each submission to a list of `top25_submissions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_submissions = []\n",
    "\n",
    "for submission in r.subreddit('news').top('year',limit=25):\n",
    "    top25_submissions.append(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first (top) `Submission` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_submission = top25_submissions[0]\n",
    "first_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `dir` function to see the other methods and attributes inside this first top `Submission` object. (There are a lot of other \"hidden\" attributes and methods that use the \"\\_\" which we can ignore with this list comprehension.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in dir(first_submission) if '_' not in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vars` may be even more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(first_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the features of each submission, store them in a dictionary, and save to an external list. This step will take a while (approximately one second per submission) because we make an API call for each submission in the `ListingGenerator` returned by the `r.subreddit('news').top('year',limit=25)` we're looping through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_stats = []\n",
    "\n",
    "for submission in r.subreddit('news').top('year',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = submission.id\n",
    "    d['title'] = submission.title\n",
    "    d['num_comments'] = submission.num_comments\n",
    "    d['score'] = submission.score\n",
    "    d['upvote_ratio'] = submission.upvote_ratio\n",
    "    d['date'] = datetime.utcfromtimestamp(submission.created_utc)\n",
    "    d['domain'] = submission.domain\n",
    "    d['gilded'] = submission.gilded\n",
    "    d['num_crossposts'] = submission.num_crossposts\n",
    "    d['nsfw'] = submission.over_18\n",
    "    if submission.author is not None:\n",
    "        d['author'] = submission.author.name\n",
    "    submission_stats.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn `submission_stats` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_df = pd.DataFrame(submission_stats)\n",
    "top25_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the relationship between score and number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top25_df.plot.scatter(x='score',y='num_comments',s=50,c='k',alpha=.5)\n",
    "# ax.set_xlim((0,200000))\n",
    "# ax.set_ylim((0,16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "This is a simple Reddit submission: [What is a dataset that you can't believe is available to the public?](https://www.reddit.com/r/datasets/comments/akb4mr/what_is_a_dataset_that_you_cant_believe_is/). We can inspect the comments in this simple submission.\n",
    "\n",
    "[Documentation for Comment model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/comment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe = r.submission(id='akb4mr')\n",
    "\n",
    "print(\"This submission was made on {0}.\".format(datetime.utcfromtimestamp(cant_believe.created_utc)))\n",
    "print(\"There are {0:,} comments.\".format(cant_believe.num_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect these comments, working from the [Comment Extraction and Parsing](https://praw.readthedocs.io/en/latest/tutorials/comments.html) tutorial in PRAW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe.comments.replace_more(limit=None)\n",
    "\n",
    "for comment in cant_believe.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each comment has a lot of metadata we can preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe_comment_metadata = []\n",
    "\n",
    "for comment in cant_believe.comments.list():\n",
    "    if not comment.collapsed: # Skip collapsed/deleted comments\n",
    "        d = {}\n",
    "        d['id'] = comment.id\n",
    "        d['parent_id'] = comment.parent_id\n",
    "        d['body'] = comment.body\n",
    "        d['depth'] = comment.depth\n",
    "        d['edited'] = comment.edited\n",
    "        d['score'] = comment.score\n",
    "        d['date'] = datetime.utcfromtimestamp(comment.created_utc)\n",
    "        d['submission_id'] = comment.submission.id\n",
    "        d['submission_title'] = comment.submission.title\n",
    "        d['subreddit'] = comment.subreddit.display_name\n",
    "        if comment.author is not None:\n",
    "            d['author'] = comment.author.name\n",
    "        cant_believe_comment_metadata.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe_df = pd.DataFrame(cant_believe_comment_metadata)\n",
    "\n",
    "# How long is the comment\n",
    "cant_believe_df['comment_length'] = cant_believe_df['body'].str.len()\n",
    "\n",
    "cant_believe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do comments deeper in this comment tree have lower scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x='depth',y='score',data=cant_believe_df,kind='bar',color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do comments deeper in this comment tree have shorter lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x='depth',y='comment_length',data=cant_believe_df,kind='bar',color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redditors\n",
    "\n",
    "A Redditor is a user and we can get meta-data about the account as well as the history of the user's comments and submissions from the API.\n",
    "\n",
    "[Documentation for the Redditor model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/redditor.html).\n",
    "\n",
    "How much link and comment karma does this user have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez = r.redditor('spez')\n",
    "print(\"Link karma: {0:,}\".format(spez.link_karma))\n",
    "print(\"Comment karma: {0:,}\".format(spez.comment_karma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Reddit flags the users who are employees of Reddit as well as if accounts have verified email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez.is_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez.has_verified_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the time this user's account was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(spez.created_utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get information about individual redditors' submissions and comment histories. Here we will use u/spez (the CEO of Reddit), get his top-voted submissions, and loop through them to get the data for each submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez_submissions = []\n",
    "\n",
    "for submission in r.redditor('spez').submissions.top('all',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = submission.id\n",
    "    d['title'] = submission.title\n",
    "    d['num_comments'] = submission.num_comments\n",
    "    d['score'] = submission.score\n",
    "    d['upvote_ratio'] = submission.upvote_ratio\n",
    "    d['date'] = datetime.utcfromtimestamp(submission.created_utc)\n",
    "    d['domain'] = submission.domain\n",
    "    d['gilded'] = submission.gilded\n",
    "    d['num_crossposts'] = submission.num_crossposts\n",
    "    d['nsfw'] = submission.over_18\n",
    "    if comment.author is not None:\n",
    "        d['author'] = submission.author.name\n",
    "    spez_submissions.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can turn this list of dictionaries into a DataFrame to do substantive data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_submissions).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get all the comments made by an editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez_comments = []\n",
    "\n",
    "for comment in r.redditor('spez').comments.top('all',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = comment.id\n",
    "    d['body'] = comment.body\n",
    "    try:\n",
    "        d['depth'] = comment.depth\n",
    "    except:\n",
    "        d['depth'] = np.nan\n",
    "    d['edited'] = comment.edited\n",
    "    d['score'] = comment.score\n",
    "    d['date'] = datetime.utcfromtimestamp(comment.created_utc)\n",
    "    d['submission_id'] = comment.submission.id\n",
    "    d['submission_title'] = comment.submission.title\n",
    "    d['subreddit'] = comment.subreddit.display_name\n",
    "    if comment.author is not None:\n",
    "        d['author'] = comment.author.name\n",
    "    spez_comments.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_comments).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This user's top comments are mostly focused in the /r/announcements subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_comments)['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
