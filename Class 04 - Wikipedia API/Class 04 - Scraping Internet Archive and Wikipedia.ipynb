{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2023 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) â€” ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping the Internet Archive and Wikipedia APIs\n",
    "* **Week 5**: Scraping the Reddit and Mastodon APIs\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "This notebook is adapted from excellent notebooks in Dr. [Cody Buntain](http://cody.bunta.in/)'s seminar on [Social Media and Crisis Informatics](http://cody.bunta.in/teaching/2018_winter_umd_inst728e/) as well as the [PRAW documentation](https://praw.readthedocs.io/en/latest/).\n",
    "\n",
    "## Class 4 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Accessing an API with `requests`\n",
    "* Retrieiving historical web data from the Internet Archive\n",
    "* Why you don't want to write a parser for Wikipedia's data\n",
    "* Fundamentals of retrieving information from Wikipedia's API\n",
    "* EDA with data from Wikipedia's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a few common libraries for all these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to other servers on the web\n",
    "import requests\n",
    "\n",
    "# APIs spit out data in JSON\n",
    "import json\n",
    "\n",
    "# Use BeautifulSoup to parse some HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Safetly quoting strings for URLs\n",
    "from urllib.parse import unquote, quote\n",
    "\n",
    "# Handling dates and times\n",
    "from datetime import datetime\n",
    "\n",
    "# DataFrames!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Internet Archive's Wayback Machine\n",
    "\n",
    "Now we'll leave some of the ethically-fraught methods of web scraping behind. The Internet Archive maintains the \"[Wayback Machine](https://www.archive.org/web/)\" where old versions of websites are stored. Some of my favorites:\n",
    "\n",
    "* [CNN in June 2000](https://web.archive.org/web/20000815052826/http://www.cnn.com/)\n",
    "* [Facebook in August 2004](https://web.archive.org/web/20040817020419/http://www.facebook.com/)\n",
    "* [Apple in April 1997](https://web.archive.org/web/19970404064444/http://www.apple.com:80/)\n",
    "\n",
    "In these URLs above, there is a numeric identifier corresponding to the timestamp when the image of the website was captured. How do we know when the Wayback Machine archived a webpage? There's a free and open API!\n",
    "\n",
    "### Using the Wayback Machine API\n",
    "\n",
    "The simplest API request we can make asks for the most recent snapshot of a webpage archived by the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response tells us the timestamp and location of this snapshot, which we could then go retrieve and parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_response_json = wb_response.json()\n",
    "\n",
    "recent_fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "recent_fb_wb_response = requests.get(recent_fb_wb_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw text out, soupify, and look for links. For some reason all the links in this snapshot are in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_fb_wb_raw = recent_fb_wb_response.text\n",
    "\n",
    "recent_fb_wb_soup = BeautifulSoup(recent_fb_wb_raw)\n",
    "\n",
    "[link.text for link in recent_fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for the most recent snapshot of a webpage around a specific date. Let's ask the Wayback Machine for a snapshot of Facebook around February 1, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com&timestamp=20080201'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response_json = wb_response.json()\n",
    "\n",
    "wb_response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a relatively deep JSON object we have to navigate into to access information like the Wayback URL or the timestamp of the snapshot. The closest snapshot to February 1, 2008 was January 30, 2008. We use the [`datetime.strptime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) function to turn this numeric string that we recognize as a timestamp into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_response_json['archived_snapshots']['closest']['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.strptime(wb_response_json['archived_snapshots']['closest']['timestamp'],\n",
    "                        '%Y%m%d%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we could scrape out the links on this 2008 version of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "fb_wb_response = requests.get(fb_wb_url)\n",
    "\n",
    "# Get the text from the response\n",
    "fb_wb_raw = fb_wb_response.text\n",
    "\n",
    "# Soup-ify\n",
    "fb_wb_soup = BeautifulSoup(fb_wb_raw)\n",
    "\n",
    "# Make a list of the text of the links\n",
    "[link.text for link in fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping historical web pages\n",
    "\n",
    "A current project I am working on is exploring how social media platforms' terms of service have evolved over time. Let's start with Facebook's terms of service and privacy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_tos = 'http://www.facebook.com/terms.php'\n",
    "fb_pp = 'http://www.facebook.com/policy.php'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take advantage of the [`date_range`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html) fuction in `pandas` to generate a range of dates between January 2005 and January 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = pd.date_range(start='2005-01-01',end='2021-03-01',freq='M')\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [`datetime.strftime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) (the inverse of `strptime`) to make these date objects into specifically-formatted strings that we can format into a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first datetime object and turn it into a string\n",
    "datetime.strftime(dates_list[0],'%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use string formatting to put the `fb_tos` URL and formatted timestamp into a request to the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.strftime(dates_list[0],'%Y%m%d')\n",
    "\n",
    "wb_api_url = 'https://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "wb_api_url_formatted = wb_api_url.format(fb_tos,date_str)\n",
    "\n",
    "print(wb_api_url_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the request to the Wayback Machine to get the URL and timestamp of the Wayback Machine's closest snapshot of Facebook's Terms of Service before January 31, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api_response = requests.get(wb_api_url_formatted)\n",
    "\n",
    "wb_api_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the markup of this old version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "wb_fb_old_url = wb_api_response.json()['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "wb_fb_raw = requests.get(wb_fb_old_url).text\n",
    "\n",
    "# Soup-ify\n",
    "wb_fb_soup = BeautifulSoup(wb_fb_raw)\n",
    "\n",
    "# Find the content element and get the text out\n",
    "wb_fb_terms_str = wb_fb_soup.find('div',{'id':'content'}).text.strip()\n",
    "\n",
    "# Inspect\n",
    "wb_fb_terms_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a really dumb stemmer, [`.split()`](https://docs.python.org/3.7/library/stdtypes.html#str.split) to count the number of words in these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wb_fb_terms_str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop to find a snapshot of Facebook's ToS each month in our `dates_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url_str,start_date='2005-01-01',end_date='2021-03-01',freq='M'):\n",
    "    \n",
    "    # Make the list of dates\n",
    "    date_l = pd.date_range(start_date,end_date,freq=freq)\n",
    "    \n",
    "    # Create an empty container to store our data\n",
    "    urls = dict()\n",
    "\n",
    "    # For each date in the list of dates\n",
    "    for date in date_l:\n",
    "        \n",
    "        # Turn the date object back into a string\n",
    "        date_str = datetime.strftime(date,'%Y%m%d%H%M%S')\n",
    "        \n",
    "        # Define the API URL request to the Wayback machine\n",
    "        wb_api_url = 'http://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "        \n",
    "        # Format the API URL with the URL of the website and the closest datetime\n",
    "        wb_api_request = wb_api_url.format(url_str,date_str)\n",
    "        \n",
    "        # Make the request\n",
    "        r = requests.get(wb_api_request).json()\n",
    "\n",
    "        # Check if the returned request has all the right parts (this is probably overkill)\n",
    "        if 'archived_snapshots' in r.keys():\n",
    "            if 'closest' in r['archived_snapshots'].keys():\n",
    "                if 'url' in r['archived_snapshots']['closest'].keys():\n",
    "                    \n",
    "                    # If it does have all the right parts, get the URL\n",
    "                    _url = r['archived_snapshots']['closest']['url']\n",
    "                    \n",
    "                    # Get the timestamp\n",
    "                    _timestamp = r['archived_snapshots']['closest']['timestamp']\n",
    "                    \n",
    "                    # Save to our URL dictionary with the timestamp of the snapshot as key, the url as value\n",
    "                    urls[_timestamp] = _url\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our function to make a dictionary of keys returning the Wayback Machine URLs for each month's version of the terms of service. We'll write a loop to get the Terms for each snapshot and count the words. \n",
    "\n",
    "This will take a few minutes. \n",
    "\n",
    "I've coverted the code block into a \"Raw\" cell to prevent accidental execution. You can always turn it into a \"Code\" cell if you really want to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of timestamps and URLs for each monthly version of the Terms of Service\n",
    "fb_terms_d = get_urls('https://www.facebook.com/terms.php')\n",
    "\n",
    "# Create an empty container to store our data\n",
    "fb_terms_wordcount = {}\n",
    "\n",
    "# Loop through the fb_terms_d dictionary\n",
    "for timestamp,url in fb_terms_d.items():\n",
    "\n",
    "    # Get the raw HTML from the Wayback Machine\n",
    "    raw = requests.get(url).text\n",
    "    \n",
    "    # Soup-ify\n",
    "    soup = BeautifulSoup(raw)\n",
    "    \n",
    "    # Find the content of the TOS\n",
    "    content = soup.find('div',{'id':'content'}).text.strip()\n",
    "    \n",
    "    # Split the content into words, count the number of words, save to the container\n",
    "    fb_terms_wordcount[timestamp] = len(content.split())\n",
    "    \n",
    "# Write to disk\n",
    "with open('facebook_tos_archive.json','w') as f:\n",
    "    json.dump(fb_terms_wordcount,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_tos_archive.json','w') as f:\n",
    "    json.dump(fb_terms_wordcount,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having everyone hit the Internet Archive server with the same requests, you can also load this file with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_tos_archive.json','r') as f:\n",
    "    fb_terms_wordcount2 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the changes in the size of Facebook's Terms of Service over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dictionary into a pandas Series\n",
    "fb_terms_s = pd.Series(fb_terms_wordcount2)\n",
    "\n",
    "# Conver the index to datetime objects\n",
    "fb_terms_s.index = pd.to_datetime(fb_terms_s.index)\n",
    "\n",
    "# Plot\n",
    "ax = fb_terms_s.plot()\n",
    "\n",
    "# Make the x-tick labels less weird\n",
    "# ax.set_xticklabels(range(2004,2021,2),rotation=0,horizontalalignment='center')\n",
    "\n",
    "# Always label your axes\n",
    "ax.set_ylabel('Word count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wikipedia\n",
    "\n",
    "Consider the Wikipedia page for [Elizabeth II](https://en.wikipedia.org/wiki/Elizabeth_II). This seems like a relatively straightforward webpage to scrape out the hyperlinks to other articles or to compare the content to other presidential biographies. However, Wikipedia also preserves the [history of every revision made to this article](https://en.wikipedia.org/w/index.php?title=Elizabeth_II&action=history) going back to the first (available) revisions in 2001, like [this](https://en.wikipedia.org/w/index.php?title=Elizabeth_II&oldid=249061). Thinking back to the Oscars example, it seems promising to find the \"oldid\" values and visit each revision's webpage to parse the content out. However, Wikipedia will give you much of this revision history data for free through its [application programming interface](http://en.wikipedia.org/w/api.php) (API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current content\n",
    "We can use `requests` to get the current HTML markup of an article from the API, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the API server lives\n",
    "query_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# An empty dictionary to store our query parameters\n",
    "query_params = {}\n",
    "\n",
    "# We want to parse the content of a page\n",
    "query_params['action'] = 'parse'\n",
    "\n",
    "# Which page?\n",
    "query_params['page'] = 'Elizabeth II'\n",
    "\n",
    "# We want the text\n",
    "query_params['prop'] = 'text'\n",
    "\n",
    "# Ignore the edit buttons and table of contents\n",
    "query_params['disableeditsection'] = 1\n",
    "query_params['disabletoc'] = 1\n",
    "\n",
    "# Get the results back as JSON\n",
    "query_params['format'] = 'json'\n",
    "\n",
    "# Format the data in an easier-to-parse option\n",
    "query_params['formatversion'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only set up our request to the API, but not sent it or received the data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = requests.get(url = query_url, params = query_params).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's waiting inside? A dictionary of dictionaries. The inner dictionary has keys for the title of the page we requested (\"George H. W. Bush\"), the pageid (a numeric identifier), and the text of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response['parse'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could count the number of links in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghwb_soup = BeautifulSoup(json_response['parse']['text'])\n",
    "\n",
    "ghwb_soup.find_all('a')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the content of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghwb_soup.find_all('p')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision history\n",
    "\n",
    "There is also an API endpoint for the revision history of this article that contains metadata about the who and when of previous changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the API server lives\n",
    "query_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# An empty dictionary to store our query parameters\n",
    "query_params = {}\n",
    "\n",
    "# We want to query properties of a page\n",
    "query_params['action'] = 'query'\n",
    "\n",
    "# Which page?\n",
    "query_params['titles'] = 'Elizabeth II'\n",
    "\n",
    "# We want the revisions\n",
    "query_params['prop'] = 'revisions'\n",
    "\n",
    "# In particular, we want the revision ids, users, comments, timestamps\n",
    "query_params['rvprop'] = 'ids|userid|comment|timestamp|user|size|sha1'\n",
    "\n",
    "# Get 500 revisions\n",
    "query_params['rvlimit'] = 500\n",
    "\n",
    "# Start old and go newer\n",
    "query_params['rvdir'] = 'newer'\n",
    "    \n",
    "# Get the results back as JSON\n",
    "query_params['format'] = 'json'\n",
    "\n",
    "# Format the data in an easier-to-parse option\n",
    "query_params['formatversion'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = requests.get(url = query_url, params = query_params).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect this `json_response`. This returns a dictionary with both \"continue\" and \"query\" keys. The continue indicates there are more than 500 revisions present in the article's history and provides an index for the next query to pick up from. The query contains the revision history we care aboutâ€”buried a bit in a nested data structure of lists and dictionaries, but we eventually get to the \"revisions\" list of dictionaries with the revision histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = json_response['query']['pages'][0]['revisions']\n",
    "revisions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = pd.DataFrame(revisions)\n",
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out how the size of the article changed over the first 500 revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rev_df.plot(y='size',legend=False)\n",
    "ax.set_ylabel('Size (bytes)')\n",
    "ax.set_xlabel('Revision')\n",
    "ax.set_xlim((0,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or count how many times an editor made a contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['user'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pageview activity\n",
    "\n",
    "There is are [API endpoints](https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews) for retrieving data on the number of times a Wikipedia article was accessed. Let's use the pageviews API ([docs](https://wikimedia.org/api/rest_v1/#/Pageviews data/)). Unfortunately, these article-level data are not stratifed by geography (by state, country, *etc*.) and are only available in the aggregate. This API only has data available back to July 2015, although there are some alternative [pageview definitions](https://meta.wikimedia.org/wiki/Research:Page_view) with data [available back to 2007](https://dumps.wikimedia.org/other/pagecounts-raw/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the string for accessing the API with some variables we'll customize\n",
    "pv_api_str = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lang}.wikipedia/all-access/user/{article}/daily/{start}/{end}\"\n",
    "\n",
    "# Customizing the pv_api_str to get information about Elizabeth II from 1 July 2015 to 1 January 2023\n",
    "retrieve_str = pv_api_str.format(\n",
    "    lang = 'en',\n",
    "    article = quote('Elizabeth II'),\n",
    "    start = '20150701',\n",
    "    end = '20230101'\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "pv_response = requests.get(retrieve_str,headers={'user-agent':'brian.keegan@colorado.edu'}).json()\n",
    "\n",
    "# Inspect\n",
    "pv_response['items'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a pandas `DataFrame` and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to a DataFrame\n",
    "e2_pv_df = pd.DataFrame(pv_response['items'])\n",
    "\n",
    "# Parse the \"timestamp\" column into a datetime\n",
    "e2_pv_df['timestamp'] = pd.to_datetime(e2_pv_df['timestamp'],format='%Y%m%d%H')\n",
    "\n",
    "# Set timestamp as index and only return reviews\n",
    "e2_pv_s = e2_pv_df.set_index('timestamp')['views']\n",
    "\n",
    "# Inspect\n",
    "e2_pv_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on log-scaled y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = e2_pv_s.plot()\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Pageviews')\n",
    "ax.set_ylim((1e4,1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function and use it in a loop to get the pageviews for each of these redirects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageviews(article_title,lang='en',start='20150701',end='20230101',headers={'user-agent':'brian.keegan@colorado.edu'}):\n",
    "    # This is the string for accessing the API with some variables we'll customize\n",
    "    _str = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lang}.wikipedia/all-access/user/{article}/daily/{start}/{end}\"\n",
    "\n",
    "    # Customizing the pv_api_str to get information about Elizabeth II from 1 July 2015 to 1 January 2023\n",
    "    _retrieve_str = _str.format(\n",
    "        lang = lang,\n",
    "        article = quote(article_title),\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    # Make the request\n",
    "    _response = requests.get(_retrieve_str,headers=headers).json()\n",
    "    \n",
    "    # Cast to a DataFrame\n",
    "    _pv_df = pd.DataFrame(pv_response['items'])\n",
    "\n",
    "    # Parse the \"timestamp\" column into a datetime\n",
    "    _pv_df['timestamp'] = pd.to_datetime(_pv_df['timestamp'],format='%Y%m%d%H')\n",
    "\n",
    "    # Set timestamp as index and only return reviews\n",
    "    _pv_s = _pv_df.set_index('timestamp')['views']\n",
    "    \n",
    "    return _pv_s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pageviews('University of Colorado Boulder').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other parts of the very powerful Wikipedia API and scraping these APIs exposes much more metadata than parsing the HTML of these webpages, while also being easier on the servers hosting it. \n",
    "\n",
    "Check out the \"Class 04 - Wikipedia functions and analysis\" notebook as well as my [`wikifunctions`](https://github.com/brianckeegan/wikifunctions/) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
