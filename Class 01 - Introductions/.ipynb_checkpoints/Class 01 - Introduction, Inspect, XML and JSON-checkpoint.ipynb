{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2023 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Course description\n",
    "\n",
    "This is a five-week one-credit \"mini-course\" on retrieving (\"scraping\") data from the web. The course is intended for researchers in the social sciences and humanities with computational instincts but limited or no prior programming experience. Each class will be 2.5 hours long: we'll take a break mid-way for biological input and output. Lectures will use a combination of lecture-by-notebook as well as hands-on exercises. The end of each class will have links to resources and additional take-home exercises. Students will have the option of presenting their solutions to the take-home exercises at the beginning of the next class.\n",
    "\n",
    "Although many programming languages offer libraries for web information retrieval and analysis, we will be focusing on the Python data analysis ecosystem given its popularity and capabilities. I would strongly recommend that students download the latest Python 3.7 or above version of the [Anaconda distribution](https://www.anaconda.com/download/) which includes the Jupyter Notebook environment we're currently in, most of the data libraries we will use, and other conveniences.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Students will:\n",
    "* Be able to navigate and access structured web data like HTML, XML, and JSON\n",
    "* Develop strategies for identifying relevant structures in semi-structed data using browser console tools\n",
    "* Utilize Python-based libraries to make request and parse web data\n",
    "* Retrieve data from platforms' application programming interfaces (APIs)\n",
    "* Critically reflect about the technological and ethical constraints on web scraping\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping the Internet Archive and Wikipedia APIs\n",
    "* **Week 5**: Scraping the Reddit and Mastodon APIs\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "To be determined based on enrollments, distribution of skills, *etc*. but will primarily involve regular attendance, participation, and upwards trajectory in skill and confidence.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks (previously called IPython Notebooks) are interactive programming environments that are great for writing, executing, and documenting code. The notebooks we'll be using launch in your web browser but are \"talking\" to a local background kernel (hence the \"localhost\" in the URL) rather than an external server. Put another way: the data you're analyzing in Jupyter Notebook isn't leaving your computer.\n",
    "\n",
    "Jupyter Notebooks are composed of \"cells\" which you can create, move, and delete. These cells can be different formats like \"Markdown\" (what this pretty cell is), \"code\" (what you'll spend most of your time in), or \"raw\" (useful for preserving code but avoiding accidentally running it). All the cells up until now have been [Markdown](https://daringfireball.net/projects/markdown/syntax), which is a lightweight standard for formatting text. You can double click on these Markdown cells to edit them.\n",
    "\n",
    "This is an example of a code cell below. You type the code into the cell and run the cell with the \"Run\" button in the toolbar or pressing Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Brian Keegan'\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebooks are great because they can keep your code, documentation, and results all in one file. Here I'll import a helper library that lets Jupyter Notebook embed images from the web (from [this tweet](https://twitter.com/barackobama/status/831527113211645959), which we'll return to in a bit). When you save this notebook, this image will be saved with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('https://pbs.twimg.com/media/C4otUykWcAIbSy1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Create at least two new cells below (before the **Additional resources** cell) and write your name and 1+1 in them.\n",
    "2. Experiment with running a code cell versus a markdown cell. How does the cell change formatting for each?\n",
    "3. Move cells around (look at the arrows in the toolbar or use the keyboard shortcuts under Help).\n",
    "4. Create a third cell and add another image from the web (use the \"Copy Image Address\" on a right click) into the notebook using the `Image` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "Jupyter Notebooks are very powerful tools that are increasingly pervasive throughout the computer, information, and data science communities. I am an [unapologetic evangelist](https://github.com/brianckeegan/Bechdel/blob/master/Bechdel_test.ipynb) for them to be used by researchers, journalists, and activists to improve documentation practices and openness in data analysis, but notebooks also have their critics: [Joel Grus](https://twitter.com/joelgrus?lang=en)'s hilarious but important presentation, [I Don't Like Notebooks](https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit). \n",
    "\n",
    "Here is some helpful documentation, tutorials, and examples; there are many others out there on the web!\n",
    "\n",
    "* [DataCamp — The Definitive Jupyter Notebook Tutorial](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Jupyter Notebook documentation](https://jupyter.readthedocs.io/en/latest/)\n",
    "* [Gallery of interesting Jupyter Notebooks](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Chrome's \"Inspect\" tool\n",
    "\n",
    "The official list of nominees and winners of the 90th Academy Awards (in 2018) is available on [the Academy of Motion Picture Arts and Sciences' website](https://www.oscars.org/oscars/ceremonies/2018). By ocular inspection, this data is at least semi-structured: there are groups of nominees, each group has a title, each nominee has the name of the movie. But this is not structured in the way that we typically analyze data, a table with rows (observations) and columns (features).\n",
    "\n",
    "This exercise assumes you're using Chrome, if you're using Safari ([instructions](https://support.apple.com/guide/safari-developer/inspecting-overview-dev1a8227029/mac)) or Mozilla ([instructions](https://developer.mozilla.org/en-US/docs/Tools/View_source)), there are slightly different workflows. If you right click on the \"Actor in a Leading Role\" and select \"Inspect\", a new window should appear on the side of your browser window. Make sure you have the \"Elements\" tab clicked.\n",
    "\n",
    "You should see a nested list of collapsable elements. This is a very complex webpage, like many are these days, with many layers of tags. However if you mouse over the elements, things highlight in the primary browser window. You can see how this is really useful for matching specific HTML codes with what they produce. You can expand an element to see its sub-elements, sub-sub-elements, and so on.\n",
    "\n",
    "You should discover that the `<div class=\"view-grouping\">` elements correspond to the nominee categories. The `\"view-grouping\"` is an attribute for the `div` tag. Expanding this element reveals a `<div class=\"view-grouping-content\">` element, which then has seven \"children\": an `<h3>` (heading 3) tag for the name of the category, an obtuse `<div>` class for the winner, another `<h3>` tag for the nominees, and then four (or more) of the same obtuse `<div>` classes for the other nominees. \n",
    "\n",
    "Expanding any one of the obtuse `<div>` elements reveals still more child tags that ultimately end with the name of the actor `<h4 class=\"field-content\">Gary Oldman</h4>` and movie `<span class=\"field-content\">Darkest Hour</span>`.\n",
    "\n",
    "We will use Chrome's \"Inspect\" tool throughout the rest of the course to find out where our data lives.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Use the inspect tool to find the HTML element that makes the triangular logo at the top of the page.\n",
    "2. Use the inspect tool to find the `<li>` tags linking to other years's awards pages.\n",
    "3. How does the HTML change under the `\"social-drawer-content\"` element when you click between Twitter, Instagram, *etc*.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms of structured data\n",
    "\n",
    "There are three primary forms of structured data you will encounter on the web: HTML, XML, and JSON.\n",
    "\n",
    "### HTML\n",
    "The Oscars example from above is an example of [HyperText Markup Language](https://www.w3.org/html/) (HTML), a markup standard that allows web browsers to render documents transmitted by a web server into webpages. HTML is distinct from—but depends strongly on—the [HyperText Transfer Protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) (HTTP) to move data from servers to clients. HTML works with other standards like Cascading Style Sheets (CSS) and JavaScript to create the dynamic and interactive websites. \n",
    "\n",
    "HTML has a tree-like structure with the \"root\" node being an `<html>` element with other elements like `<body>`s and `<head>`s as children, and those children having other children live the `<div>`, `<a>`, and [other elements](https://en.wikipedia.org/wiki/HTML_element). See the visualization below for a simple example of an HTML tree.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day2-digital-trace-data/screenscraping/rmarkdown/html_tree.png\"></img>\n",
    "<em>From Chris Bail's \"Screen-Scraping in R\": <a href=\"https://cbail.github.io/SICSS_Screenscraping_in_R.html\">https://cbail.github.io/SICSS_Screenscraping_in_R.html</a></em>\n",
    "\n",
    "It's beyond the scope of this mini-course to cover what all these elements do or how they interact—I'll leave it to you to develop some folk theories of these tags' functionality based on exploring pages with the \"Inspect\" tool above.\n",
    "\n",
    "We can use Python's `requests` library to make a valid HTTP \"get\" request to the Oscars' web server for the 90 Academy Awards which will return the raw HTML. There are more than 144,000 characters in the document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Pretend to be a web browser and make a get request of a webpage\n",
    "oscars90_request = requests.get('https://www.oscars.org/oscars/ceremonies/2018')\n",
    "\n",
    "# The .text returns the text from the request\n",
    "oscars90_html = oscars90_request.text\n",
    "\n",
    "# The oscars90_html is a string, we can use the common len function to ask how long the string is (in characters)\n",
    "len(oscars90_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first thousand characters. Mostly declarations to handle Internet Explorer's notorious refusal to follow web standards—stuff you don't need to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The [0:1000] is a slicing notation \n",
    "# It gets the first (position 0 in Python) character until the 1000th character\n",
    "\n",
    "print(oscars90_html[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at 1,000 lines about a third of the way through the document, we can see some of the structure we found with the \"Inspect\" tool above corresponding to the closing lines of the \"Actor in a Supporting Role\" grouping and the opening lines of the \"Acress in a Leading Role\" grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can slice any ranges you'd like up, as long as it's not beyond the length of the string\n",
    "# oscars90_html[144588:] would return an error\n",
    "\n",
    "print(oscars90_html[50000:51000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not actually going to be slicing the text to get this structured data out, we'll use a wonderful tool call [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to do the heavy lifting for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML\n",
    "\n",
    "The HTML from the example above is too obtuse to start working with BeautifulSoup. The [Extensible Markup Language](https://www.w3.org/XML/) (XML) is a related markup language for representing data structures. XML was all the rage at the turn of the century: \"many software designers can barely contain their excitement over its potential to establish a real Internet lingua franca\" (*The New York Times* in 2000: \"[The Next Big Step? It's Called XML](https://www.nytimes.com/2000/06/07/business/the-next-big-leap-it-s-called-xml.html)\"). That obviously did not come to pass. But XML remains a robust and open—though verbose—standard for representing structured data.\n",
    "\n",
    "XML has taken on something of an afterlife as the official data standard for the U.S. Congress. The [House](http://clerk.house.gov/index.aspx) and [Senate](https://www.senate.gov/general/XML.htm) both release information about members, committees, schedules, legislation, and votes in XML. These are immaculately formatted and documented and remarkably up-to-date: the data for members of the 116th Congress are already posted.\n",
    "\n",
    "Use the `requests` library to make a HTTP get request to the House's webserver and get the list of current member data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_raw = requests.get('http://clerk.house.gov/xml/lists/MemberData.xml').text\n",
    "\n",
    "senate_raw = requests.get('https://www.senate.gov/legislative/LIS_MEMBER/cvc_member_data.xml').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is still in a string format (`type(house_raw)`), so it's difficult to search and navigate. Let's make our first soup together using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the library\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Then make the soup, specifying the \"lxml\" parser\n",
    "house_soup = BeautifulSoup(house_raw,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's so great about this soup-ified string? We now have a suite of new functions and methods that let us navigate the tree. First, let's inspect the different tags/elements in this tree of House member data. This is the full tree of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store data\n",
    "children = []\n",
    "\n",
    "# Start a loop to go through all the children tags in house_soup\n",
    "for tag in house_soup.findChildren():\n",
    "    \n",
    "    # If the name of the tag (tag.name) is not already in the children list\n",
    "    if tag.name not in children:\n",
    "        \n",
    "        # Add the name of the tag to the children list\n",
    "        children.append(tag.name)\n",
    "\n",
    "# Look at the list members\n",
    "children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can navigate through the tree. You won't do this in practice, but it's helpful for debugging. In this case, we navigated from the root node (`html`) into the `body` tag, then the `memberdata` tag, then the `members` tag. There are 441 descendents at this level, corresponding to the 435 voting seats and the 6 seats for territories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(house_soup.html.body.memberdata.members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also short-cut to the members tag directly rather than navigating down the parent elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(house_soup.members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.contents` method is great for getting a list of the children below the tag as a list. We can use the `[0]` slice to get the first member and their data in the list. Interestingly, the `<committee-assignments>` tags are currently empty since these have not yet been allocated, but will in the next few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.members.contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could keep navigating down the tree from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.members.contents[0].bioguideid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this navigation method breaks when the tag has a hyphen in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.members.contents[0].state-fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead you can use the `.find()` method to handle these hyphenated cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.members.contents[0].find('state-fullname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the text inside the tag with `.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.members.contents[0].find('state-fullname').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find_all()` method will be your primary tool when working with structured data. The `<party>` tag codes party membership (D=Democratic, R=Republican) for each representative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_soup.find_all('party')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There [should be](https://en.wikipedia.org/wiki/116th_United_States_Congress#Party_summary) 235 Democrats and 199 Republicans, plus the other non-voting members from territories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter\n",
    "democrats = 0\n",
    "republicans = 0\n",
    "other = 0\n",
    "\n",
    "# Loop through each element of the caucus tags\n",
    "for p in house_soup.find_all('party'):\n",
    "    \n",
    "    # Check if it's D, R, or something else\n",
    "    if p.text == \"D\":\n",
    "        \n",
    "        # Increment the appropriate counter\n",
    "        democrats += 1\n",
    "    \n",
    "    elif p.text == \"R\":\n",
    "        republicans += 1\n",
    "    else:\n",
    "        other += 1\n",
    "        \n",
    "print(\"There are {0} Democrats, {1} Republicans, and {2} others in the 116th Congress.\".format(democrats,republicans,other))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How many members serve in \"At Large\" districts? (There's at least two ways to do this)\n",
    "2. How many members are women?\n",
    "3. How many members are not incumbents? (They were not in the prior Congress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "HTML in the wild can be incredibly complicated (which is why we'll dedicate two whole class sessions to it) and XML is verbose and has other ideosyncracies (which is probably why we won't see it after this class). [JavaScript Object Notation](https://www.json.org/) (JSON) is probably the most popular data markup language and is especially ubiquitous when retreiving data from the application programming interfaces (APIs) of popular platforms like Twitter, Reddit, Wikipedia, *etc*.\n",
    "\n",
    "JSON is attractive for programmers using JavaScript and Python because it can represent a mix of different data types. We need to make a brief digression into Python's fundamental data stuctures in order to understand the contemporary attraction to JSON. Python has a few fundamental data types for representing collections of information:\n",
    "\n",
    "* **Lists**: This is a basic ordered data structure that can contain strings, ints, and floats.\n",
    "* **Dictionaries**: This is an unordered data structure containing key-value pairs, like a phonebook.\n",
    "\n",
    "Here are some basic examples of lists. Note that lists can contain other lists, and lists of lists of lists, and so on. In this case `school_district` is a list of schools, where each school is a list of classrooms, where each classroom is a list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classrooms as lists with student names as strings\n",
    "classroom0 = ['Alice','Bob','Carol','Dave']\n",
    "classroom1 = ['Eve','Frank','Grace','Harold']\n",
    "classroom2 = ['Isabel','Jack','Katy','Lloyd']\n",
    "classroom3 = ['Maria','Nate','Olivia','Philip']\n",
    "classroom4 = ['Quinn','Rachel','Steve','Terry','Ursula']\n",
    "classroom5 = ['Violet','Walter','Xavier','Yves','Zoe']\n",
    "\n",
    "# Make schools that contain classrooms\n",
    "school0 = [classroom0,classroom1]\n",
    "school1 = [classroom2,classroom3]\n",
    "school2 = [classroom4,classroom5]\n",
    "\n",
    "# Make a school district that contains schools\n",
    "school_district = [school0,school1,school2]\n",
    "\n",
    "# Inspect one school's enrollments\n",
    "school_district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access list elements by their position. In this case, we could access the first name in `classroom0` by requesting the item at the 0th index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classroom0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some basic examples of dictionaries. Note that dictionaries can contain other dictionaries, and so on. In this example `mountain_west` is a dictionary of dictionaries where the outer dictionary is keyed by the name of the state and the inner dictionary has different statistical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of states containing dictionaries\n",
    "mountain_west = {\n",
    "    'Colorado': {\n",
    "        'Abbreviation': 'CO',\n",
    "        'Area': 269601,\n",
    "        'Capital': 'Denver',\n",
    "        'Established': '1876-08-01',\n",
    "        'Largest city': 'Denver',\n",
    "        'Population': 5540545,\n",
    "        'Representatives': 8\n",
    "    },\n",
    "    'Idaho': {\n",
    "        'Abbreviation': 'ID',\n",
    "        'Area': 216443,\n",
    "        'Capital': 'Boise',\n",
    "        'Established': '1890-07-03',\n",
    "        'Largest city': 'Boise',\n",
    "        'Population': 1683140,\n",
    "        'Representatives': 2\n",
    "    },\n",
    "    'Montana': {\n",
    "        'Abbreviation': 'MT',\n",
    "        'Area': 380831,\n",
    "        'Capital': 'Helena',\n",
    "        'Established': '1889-11-08',\n",
    "        'Largest city': 'Billings',\n",
    "        'Population': 1042520,\n",
    "        'Representatives': 2\n",
    "    },\n",
    "    'Utah': {\n",
    "        'Abbreviation': 'UT',\n",
    "        'Area': 219882,\n",
    "        'Capital': 'Salt Lake City',\n",
    "        'Established': '1896-01-04',\n",
    "        'Largest city': 'Salt Lake City',\n",
    "        'Population': 3051217,\n",
    "        'Representatives': 4\n",
    "    },\n",
    "    'Wyoming': {\n",
    "        'Abbreviation': 'WY',\n",
    "        'Area': 253335,\n",
    "        'Capital': 'Cheyenne',\n",
    "        'Established': '1890-07-10',\n",
    "        'Largest city': 'Cheyenne',\n",
    "        'Population': 585501,\n",
    "        'Representatives': 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the keys and what are the values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_west.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_west.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You access dictionary values by their keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about Colorado\n",
    "mountain_west['Colorado']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the population of Colorado? In a dictionary of dictionaries, you can access the inner dictionary with another key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_west['Colorado']['Population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested data structures do not need to be the same type. A dictionary can have a list as a value for example. This would probably make the `school_district` example easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_district_dict = {\n",
    "    'School 1':{\n",
    "        'Classroom A':['Alice','Bob','Carol','Dave'],\n",
    "        'Classroom B':['Eve','Frank','Grace','Harold']\n",
    "    },\n",
    "    'School 2':{\n",
    "        'Classroom A':['Isabel','Jack','Katy','Lloyd'],\n",
    "        'Classroom B':['Maria','Nate','Olivia','Philip']\n",
    "    },\n",
    "    'School 3':{\n",
    "        'Classroom A':['Quinn','Rachel','Steve','Terry','Ursula'],\n",
    "        'Classroom B':['Violet','Walter','Xavier','Yves','Zoe']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the 3rd member (remember Python indexes start at 0, so we ask for 2) of \"Classroom B\" in \"School 2\" by working our way outside inward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_district_dict['School 2']['Classroom B'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list can take just about any other data structure as a value. Dictionaries are great because they label your data for you but don't preserve order and need unique keys, so they're bad at storing repeated data like a time series. Here's a list where the values are dictionaries and each dictionary is information about the population, state, and year of a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population = [ \n",
    "    {'Population': 4338785, 'State': 'Colorado', 'Year': 2000},\n",
    "    {'Population': 4444513, 'State': 'Colorado', 'Year': 2001},\n",
    "    {'Population': 4504709, 'State': 'Colorado', 'Year': 2002},\n",
    "    {'Population': 4555084, 'State': 'Colorado', 'Year': 2003},\n",
    "    {'Population': 4608811, 'State': 'Colorado', 'Year': 2004},\n",
    "    {'Population': 4662534, 'State': 'Colorado', 'Year': 2005},\n",
    "    {'Population': 4745660, 'State': 'Colorado', 'Year': 2006},\n",
    "    {'Population': 4821784, 'State': 'Colorado', 'Year': 2007},\n",
    "    {'Population': 4901938, 'State': 'Colorado', 'Year': 2008},\n",
    "    {'Population': 4976853, 'State': 'Colorado', 'Year': 2009},\n",
    "    {'Population': 5049935, 'State': 'Colorado', 'Year': 2010},\n",
    "    {'Population': 5119538, 'State': 'Colorado', 'Year': 2011},\n",
    "    {'Population': 5191086, 'State': 'Colorado', 'Year': 2012},\n",
    "    {'Population': 5268413, 'State': 'Colorado', 'Year': 2013},\n",
    "    {'Population': 5350118, 'State': 'Colorado', 'Year': 2014},\n",
    "    {'Population': 5448055, 'State': 'Colorado', 'Year': 2015},\n",
    "    {'Population': 5538180, 'State': 'Colorado', 'Year': 2016}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the population of Colorado in 2008?\n",
    "\n",
    "It's actually a small pain to get it out in this list of dictionaries format. You could loop through each dictionary in the list and ask if its \"Year\" in 2008 and only then print out its \"Population\". You could discover the min and max years (assuming the data is reliably chronological) and then calculate the index where 2008 should occur. This format probably starts to remind you of some of the limitations of XML, but it has strengths in some situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each dictionary in the list\n",
    "for year_data in state_population:\n",
    "    \n",
    "    # If the inner dictionary's value for \"Year\" is 2008, do something, otherwise skip\n",
    "    if year_data['Year'] == 2008:\n",
    "        \n",
    "        # Print our the inner dictionary's value for \"Population\"\n",
    "        print(year_data['Population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2000 is at index 0, so year 2008 should be at index 8\n",
    "# Passing only the index 8 returns a dictionary\n",
    "# Include the key whose value you want from this dictionary: \"Population\"\n",
    "state_population[8]['Population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Below is an example of a [tweet status](https://dev.twitter.com/overview/api/tweets) object that Twitter's [API returns](https://dev.twitter.com/rest/reference/get/statuses/show/id). This `obama_tweet` dictionary corresponds to [this tweet](https://twitter.com/BarackObama/status/831527113211645959). This is a classic example of a JSON object containing a mixture of dictionaries, lists, lists of dictionaries, dictionaries of lists, *etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_tweet = {'created_at': 'Tue Feb 14 15:34:47 +0000 2017',\n",
    "               'favorite_count': 1023379,\n",
    "               'hashtags': [],\n",
    "               'id': 831527113211645959,\n",
    "               'id_str': '831527113211645959',\n",
    "               'lang': 'en',\n",
    "               'media': [{'display_url': 'pic.twitter.com/O0UhJWoqGN',\n",
    "                          'expanded_url': 'https://twitter.com/BarackObama/status/831527113211645959/photo/1',\n",
    "                          'id': 831526916398149634,\n",
    "                          'media_url': 'http://pbs.twimg.com/media/C4otUykWcAIbSy1.jpg',\n",
    "                          'media_url_https': 'https://pbs.twimg.com/media/C4otUykWcAIbSy1.jpg',\n",
    "                          'sizes': {'large': {'h': 800, 'resize': 'fit', 'w': 1200},\n",
    "                                    'medium': {'h': 800, 'resize': 'fit', 'w': 1200},\n",
    "                                    'small': {'h': 453, 'resize': 'fit', 'w': 680},\n",
    "                                    'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
    "                          'type': 'photo',\n",
    "                          'url': 'https://t.co/O0UhJWoqGN'}],\n",
    "               'retweet_count': 252266,\n",
    "               'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
    "               'text': 'Happy Valentine’s Day, @michelleobama! Almost 28 years with you, but it always feels new. https://t.co/O0UhJWoqGN',\n",
    "               'urls': [],\n",
    "               'user': {'created_at': 'Mon Mar 05 22:08:25 +0000 2007',\n",
    "                        'description': 'Dad, husband, President, citizen.',\n",
    "                        'favourites_count': 10,\n",
    "                        'followers_count': 84814791,\n",
    "                        'following': True,\n",
    "                        'friends_count': 631357,\n",
    "                        'id': 813286,\n",
    "                        'lang': 'en',\n",
    "                        'listed_count': 221906,\n",
    "                        'location': 'Washington, DC',\n",
    "                        'name': 'Barack Obama',\n",
    "                        'profile_background_color': '77B0DC',\n",
    "                        'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/451819093436268544/kLbRvwBg.png',\n",
    "                        'profile_banner_url': 'https://pbs.twimg.com/profile_banners/813286/1484945688',\n",
    "                        'profile_image_url': 'http://pbs.twimg.com/profile_images/822547732376207360/5g0FC8XX_normal.jpg',\n",
    "                        'profile_link_color': '2574AD',\n",
    "                        'profile_sidebar_fill_color': 'C2E0F6',\n",
    "                        'profile_text_color': '333333',\n",
    "                        'screen_name': 'BarackObama',\n",
    "                        'statuses_count': 15436,\n",
    "                        'time_zone': 'Eastern Time (US & Canada)',\n",
    "                        'url': 'https://t.co/93Y27HEnnX',\n",
    "                        'utc_offset': -18000,\n",
    "                        'verified': True},\n",
    "               'user_mentions': [{'id': 409486555,\n",
    "                                  'name': 'Michelle Obama',\n",
    "                                  'screen_name': 'MichelleObama'}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the top-most keys in the `obama_tweet` object?\n",
    "2. When was this tweet sent?\n",
    "3. Does this tweet mention anyone?\n",
    "4. How many retweets did this tweet receive (at the time I collected it)?\n",
    "5. How many followers does the \"user\" who wrote this tweet have?\n",
    "6. What's the \"media_url\" for the image in this tweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical web scraping\n",
    "\n",
    "The phrase \"data scraping\" is colloquial and popular but has pejorative connotations. Data is valuable: other people invested time in collecting, organizing, and sharing it. When you show up with a scraper you built after maybe a dozen hours demanding data, you rarely pay the costs of labor, hosting, *etc*. that went into making the data available. There are *very* good rationales for making many kinds of data more availabile: reproducibility of scientific results, sharing publicly-funded and/or close-to-zero marginal cost resources, transparency and accountability in democratic institutions, remixing for innovative new analyses, *etc*. \n",
    "\n",
    "But data breaches have become eponymous (Target in 2013, Equifax in 2017, Facebook in 2018, *etc*.) because they violate other values like privacy. These manifest most clearly in principles outlined in the 1978 [Belmont Report](https://en.wikipedia.org/wiki/Belmont_Report):\n",
    "* **Respect for persons**: protecting the autonomy of all people and treating them with courtesy and respect and allowing for informed consent. Researchers must be truthful and conduct no deception;\n",
    "* **Beneficence**: The philosophy of \"Do no harm\" while maximizing benefits for the research project and minimizing risks to the research subjects; and\n",
    "* **Justice**: ensuring reasonable, non-exploitative, and well-considered procedures are administered fairly — the fair distribution of costs and benefits to potential research participants — and equally.\n",
    "\n",
    "(A fourth principle \"Respect for Public\" emphasizes compliance, accountability, and transparency in the conduct of research.)\n",
    "\n",
    "In the context of data scraping, there are four \"areas of difficulty\":\n",
    "\n",
    "* **Informed consent**: does the data scraper obtain consent from every person whose data is being retrieved?\n",
    "* **Informational risk**: can the data scraper inflict economic, social, *etc*. harm on individuals by disclosing data?\n",
    "* **Privacy**: does the data scraper know which information a person intended to be private or public? \n",
    "* **Decision-making under uncertainty**: does the data scraper know all the ways the data could be (mis)used? \n",
    "\n",
    "Ethical and legal risks involved with scraping:\n",
    "\n",
    "* **[Copyright infringement](https://en.wikipedia.org/wiki/Copyright_infringement)**: compiling data that someone else can claim ownership over\n",
    "* **[Trespass](https://en.wikipedia.org/wiki/Trespass_to_chattels#In_the_electronic_age)**: over-aggressive scraping shuts down someone else's property\n",
    "* **[Computer Fraud & Abuse Act](https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act)**: misrepresenting yourself to access a system is \"hacking\"\n",
    "\n",
    "While I cannot provide legal advice, we will revisit these concerns throughout the course through best practices for avoiding infringement, staggering data collection, simulating human requests, securing data, and protecting privacy.\n",
    "\n",
    "James Densmore has a nice summary of [practices for ethical web scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
    "\n",
    "> * If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "> * I will always provide a User Agent string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "> * I will request data at a reasonable rate. I will strive to never be confused for a DDoS attack.\n",
    "> * I will only save the data I absolutely need from your page. If all I need it OpenGraph meta-data, that’s all I’ll keep.\n",
    "> * I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "> * I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "> * I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "> * I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
    "\n",
    "Some other important components of ethical web scraping practices [include](http://robertorocha.info/on-the-ethics-of-web-scraping/):\n",
    "\n",
    "* Reading the Terms of Service and Privacy Policies for the site's rules on scraping.\n",
    "* Inspecting the robots.txt file for rules about what pages can be scraped, indexed, *etc*.\n",
    "* Be gentle on smaller websites by running during off-peak hours and spacing out requests.\n",
    "* Identify yourself by name and email in your User-Agent strings\n",
    "\n",
    "What does a robots.txt file look like? Here is CNN's. It helpfull provides a sitemap to the robot to get other pages, it allows all kinds of User-agents, and disallows crawling of pages in specific directories (ads, polls, tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get('https://www.cnn.com/robots.txt').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are scraping websites, it is a good idea to include your contact information as a custom User-Agent string so that the webmaster can get in contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_header = {'User-Agent':'Python research tool by Brian Keegan, brian.keegan@colorado.edu'}\n",
    "\n",
    "request = requests.get('https://www.cnn.com',headers=contact_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adverse consequences of web scraping include:\n",
    "* Compromising the privacy and integrity of individual users' data\n",
    "* Damaging a web server with too many requests\n",
    "* Denying access to the web service to other authorized users\n",
    "* Infringing on copyrighted material\n",
    "* Damaging the business value of a web site\n",
    "\n",
    "[Amanda Bee](http://velociraptor.info/) compiled [a nice set of examples](https://github.com/amandabee/scraping-for-journalists/wiki/Reporting-Examples) of data journalists using web scraping for their reporting. There are some ethical justifications for violating a site's terms of service to scrape data:\n",
    "* Obtaining data for the public interest from official statements, government reports, *etc*.\n",
    "* Conducting audit studies (as long as these are responsibly designed and pre-cleared)\n",
    "* The data is unavailable from APIs, FOIA requests, and other reports\n",
    "\n",
    "[Sophie Chou](http://sophiechou.com/) made this nice [decision flow-chart](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/) of whether to build a scraper or not from a NICAR panel in 2016:\n",
    "\n",
    "![Should you build a scraper flowchart](http://www.storybench.org/wp-content/uploads/2016/04/flowchart_final.jpeg)\n",
    "\n",
    "Why is there a \"Talk to a lawyer?\" outcome at the bottom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Fraud and Abuse Act\n",
    "\n",
    "The [Computer Fraud and Abuse Act](https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act) was passed in 1984, [in large part due to](https://www.cnet.com/news/from-wargames-to-aaron-swartz-how-u-s-anti-hacking-law-went-astray/) the 1983 film [WarGames](https://en.wikipedia.org/wiki/WarGames) starring Matthew Broderick. A plain reading of the text of the law ([18 U.S.C. § 1030](https://www.law.cornell.edu/uscode/text/18/1030)) criminalizes just about any form of web scraping:\n",
    "\n",
    "> * Whoever intentionally accesses a computer without authorization or exceeds authorized access, and thereby obtains… information from any protected computer;\n",
    "> * knowingly causes the transmission of a program, information, code, or command, and as a result of such conduct, intentionally causes damage without authorization, to a protected computer;\n",
    "> * the term “exceeds authorized access” means to access a computer with authorization and to use such access to obtain or alter information in the computer that the accesser is not entitled so to obtain or alter;\n",
    "> * the term “damage” means any impairment to the integrity or availability of data, a program, a system, or information;\n",
    "> * the term “protected computer” means a computer which is used in or affecting interstate or foreign commerce or communication, including a computer located outside the United States that is used in a manner that affects interstate or foreign commerce or communication of the United States;\n",
    "\n",
    "Violators can be fined and jailed under a misdemeanor charge for up to 1 year for the first violation and jailed up to 10 years under a felony charge for repeated violations.\n",
    "\n",
    "This law has a [chilling effect](https://en.wikipedia.org/wiki/Chilling_effect) on many forms of research, journalism, and other forms of protected speech. The CFAA has been used by federal prosecutors to bring federal felony charges against programmers, journalists, and activists. In 2011, programmer and hacktivist [Aaron Swartz](https://en.wikipedia.org/wiki/Aaron_Swartz) (who contributed to the development of RSS, Markdown, Creative Commons, and Reddit) was [arrested and charged](https://en.wikipedia.org/wiki/United_States_v._Swartz) with violating the CFAA for downloading several million PDFs from JSTOR over MIT's network. The [decision to prosecute was unusual](https://www.huffingtonpost.com/2013/03/13/aaron-swartz-prosecutorial-misconduct_n_2867529.html). Facing 35 years of imprisonment and over $1 million in fines under the CFAA, Swartz committed suicide on January 11, 2013.\n",
    "\n",
    "In 2016, four computer science researchers and the publisher of *The Intercept* who all use scraping techniques to run experiments to measure bias and discrimination in web content [filed suit with the ACLU](https://www.aclu.org/cases/sandvig-v-sessions-challenge-cfaa-prohibition-uncovering-racial-discrimination-online) against the U.S. Government: *Sandvig v. Sessions*. Their research involves creating multiple fake accounts, providing inaccurate information to websites, using automated tools to record publicly-available data, and other scraping techniques. In March 2018, the [D.C. Circuit Court ruled](https://www.aclu.org/news/judge-allows-aclu-case-challenging-law-preventing-studies-big-data-discrimination-proceed) two of the plantiffs have standing to sue and the case is currently being prepared for trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
