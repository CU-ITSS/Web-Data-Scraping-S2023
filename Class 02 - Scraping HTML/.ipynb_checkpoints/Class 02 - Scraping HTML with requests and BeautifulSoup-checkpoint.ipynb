{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2023 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) â€” ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping the Internet Archive and Wikipedia APIs\n",
    "* **Week 5**: Scraping the Reddit and Mastodon APIs\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "## Class 2 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Parsing HTML data into tabular data\n",
    "* Writing your own parser\n",
    "* Traversing directories vs. parsing targets to retrieve data\n",
    "* Applying techniques and debugging for individual projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing accomplishments and challenges\n",
    "\n",
    "* Using the inspect tool\n",
    "* Counting numbers of members of U.S. House with XML\n",
    "* Parsing information out from Twitter's JSON payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML data into tabular data\n",
    "\n",
    "The overall goal we have as researchers in scraping data from the web is converting data from one structured format (HTML's tree-like structures) into another structured format (probably a tabular structure with rows and columns). \n",
    "This could involve simply reading tables out of a webpage all the way up to taking irregularly-structured HTML elements into a tabular format. \n",
    "\n",
    "We are going to make some use of the [`pandas`](https://pandas.pydata.org/) library (\"**pan**el **da**ta\", not the cute animal), which is Python's implementation of a data frame concept. This is a very powerful and complex library that I typically spend more than 12 hours of lecture teaching in intermediate programming classes. I hope to convey some important elements as we work through material, but it is far beyond the scope of this class to be able to cover all the fundamentals and syntax. \n",
    "\n",
    "Let's begin by importing the libraries we'll need in this notebook: requests, BeautifulSoup, and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most straight-forward way to import a librayr in Python\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup is a module inside the \"bs4\" library, we only import the BeautifulSoup module\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We import pandas but give the library a shortcut alias \"pd\" since we will call its functions so much\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading an HTML table into Python\n",
    "\n",
    "[The Numbers](http://www.the-numbers.com) is a popular source of data about movies' box office revenue numbers. Their daily domestic charts are HTML tables with the top-grossing movies for each day of the year, going back for several years. This [table](https://www.the-numbers.com/box-office-chart/daily/2018/12/25) for Christmas day in 2018 has coluns for the current week's ranking, previous week's ranking, name of movie, distributor, gross, change over the previous week, number of theaters, revenue per theater, total gross, and number of days since release. This looks like a fairly straightforward table that could be read directly into data frame-like structure.\n",
    "\n",
    "Using the Inspect tool, we can see the table exists as a `<table border=\"0\" ... align=\"CENTER\">` element with child tags like `<tbody>` and `<tr>` (table row). Each `<tr>` has `<td>` which defines each of the cells and their content. For more on how HTML defines tables, check out [this tutoral](https://www.w3schools.com/html/html_tables.asp).\n",
    "\n",
    "Using `requests` and `BeautifulSoup` we would get this webpage's HTML, turn it into soup, and then find the table (`<table>`) or the table rows (`<tr>`) and pull out their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "xmas_bo_raw = requests.get('https://www.the-numbers.com/box-office-chart/daily/2018/12/25').text\n",
    "\n",
    "# Turn into soup, specify the HTML parser\n",
    "xmas_bo_soup = BeautifulSoup(xmas_bo_raw,'html.parser')\n",
    "\n",
    "# Use .find_all to retrieve all the tables in the page\n",
    "xmas_bo_tables = xmas_bo_soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there are two tables on the page, the first is a baby table consisting of the \"Previous Chart\", \"Chart Index\", and \"Next Chart\" at the top. We want the second table with all the data: `xmas_bo_tables[1]` returns the second chart (remember that Python is 0-indexed, so the first chart is at `xmas_bo_tables[0]`). With this table identified, we can do a second `find_all` to get the table rows inside it and we save it as `xmas_bo_trs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs = xmas_bo_tables[1].find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a few of these rows. The first row in our list of rows under `xmas_bo_trs` should be the header with the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table row should be for Aquaman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to access the contents of this table row, we could use the `.contents` method to get a list of each of the `<td>` table cells, which (frustratingly) intersperses newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs[1].contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is to use the `.text` method to get the text content of all the cells in this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\\n` characters re-appear here, but if we `print` out this statement, we see their newline functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xmas_bo_trs[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use string processing to take this text string and convert it into a simple list of data. `.split('\\n')` will split the string on the newline characters and return a list of what exists in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_trs[1].text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a `for` loop to go through all the table rows in `xmas_bo_trs`, get the list of data from the row, and add it back to a list of all the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_xmas_bo_rows = []\n",
    "\n",
    "# Loop through all the non-header (first row) table rows\n",
    "for row in xmas_bo_trs[1:]:\n",
    "    \n",
    "    # Get the text of the row and split on the newlines (like above)\n",
    "    cleaned_row = row.text.split('\\n')\n",
    "    \n",
    "    # Add this cleaned row back to the external list of row data\n",
    "    cleaned_xmas_bo_rows.append(cleaned_row)\n",
    "    \n",
    "# Inspect the first few rows of data\n",
    "cleaned_xmas_bo_rows[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass this list of lists in `cleaned_xmas_bo_rows` to pandas's `DataFrame` function and hopefully get a nice table out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_df = pd.DataFrame(cleaned_xmas_bo_rows)\n",
    "\n",
    "# Inspect\n",
    "xmas_bo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a bit of cleanup on this data:\n",
    "\n",
    "* Columns 0 and 11 are all empty\n",
    "* Add column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 0 and 11 and overwrite the xmas_box_df variable\n",
    "xmas_bo_df = xmas_bo_df.drop(columns=[0,11])\n",
    "\n",
    "# Rename the columns\n",
    "xmas_bo_df.columns = ['Rank','Last rank','Movie','Distributor','Gross',\n",
    "                      'Change','Theaters','Per theater','Total gross',\n",
    "                      'Days']\n",
    "\n",
    "# Write to disk\n",
    "# xmas_bo_df.to_csv('christmas_2018_box_office.csv',encoding='utf8')\n",
    "\n",
    "# Inspect\n",
    "xmas_bo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pandas`'s `read_html`\n",
    "That was a good amount of work just to get this simple HTML table into Python. But it was important to cover how table elements moved from a string in `requests`, into a soup object from `BeautifulSoup`. into a list of data, and finally into `pandas`. \n",
    "\n",
    "`pandas` also has powerful functionality for reading tables directly from HTML. If we convert the soup of the first table (`xmas_bo_tables[1]`) back into a string, `pandas` can read it directly into a table. \n",
    "\n",
    "There are a few ideosyncracies here, the result is a list of dataframesâ€”even if there's only a single table/dataframeâ€”so we need to return the first (and only) element of this list. This is why there's a `[0]` at the end and the `.head()` is just to show the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_bo_table_as_string = str(xmas_bo_tables[1])\n",
    "\n",
    "pd.read_html(xmas_bo_table_as_string)[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names got lumped in as rows, but we can fix this as well with the `read_html` function by passing the row index where the column lives. In this case, it is the first row, so we pass `header=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html(xmas_bo_table_as_string,header=0)[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can point `read_html` at a URL without any `requests` or `BeautifulSoup` and get all the tables on the page as a list of DataFrames. `pandas` is simply doing the `requests` and `BeautifulSoup` on the inside. Interestingly, I'm getting a [HTTP 403](https://en.wikipedia.org/wiki/HTTP_403) error indicating the server (The Numbers) is forbidding the client (us) from accessing their data using this strategy. We will discuss next week whether and how to handle situations where web servers refuse connections from non-human clients. In this case, you cannot use the off-the-shelf `read_html` approach and would need to revert to using the `requests`+`BeautifulSoup` approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tables = pd.read_html('https://www.the-numbers.com/box-office-chart/daily/2018/12/25')\n",
    "simple_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we point it at Wikipedia's [2018 in film](https://en.wikipedia.org/wiki/2018_in_film), it will pull all of the tables present on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tables = pd.read_html('https://en.wikipedia.org/wiki/2018_in_film')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three correspond to the \"Year in film\" navigation box on the side and are poorly-formatted by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth table in the `simple_tables` list we got from parsing the Wikipedia page with `read_html` is the table under the \"Highest-grossing films\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tables[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the \"header\" option in `read_html` to make sure the column names from a particular row (in this case the first row) do not accidentally become rows of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_top_grossing_t = pd.read_html('https://en.wikipedia.org/wiki/2018_in_film',header=0)[3]\n",
    "wiki_top_grossing_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are still a few errors in this table because the \"Disney\" value in the Wikipedia table spans two rows and `read_html` thus skips the \"Distributor\" value for Black Panther. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the value at index position 1, column position Distributor to Wordwide gross\n",
    "wiki_top_grossing_t.loc[1,'Worldwide gross'] = wiki_top_grossing_t.loc[1,'Distributor']\n",
    "\n",
    "# Change the value at 1, Distributor to Disney\n",
    "wiki_top_grossing_t.loc[1,'Distributor'] = 'Disney'\n",
    "\n",
    "wiki_top_grossing_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing your own parser\n",
    "\n",
    "We will return to the historical Oscars data. Even though data as prominent as this is likely to already exist in tabular format somewhere, we will maintain the illusion that we are the first to both scrape it and parse it into a tabular format. Our goal here is to write a parser that will (ideally) work across multiple pages; in this case, each of the award years.\n",
    "\n",
    "One of the first things we should do before writing any code is come up with a model of what we want our data to look like at the end of this. This is an intuitive and \"tidy\" format, but you might come up with alternatives based on your analysis and modeling needs.\n",
    "\n",
    "| *Year* | *Category* | *Nominee* | *Movie* | *Won* |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2019 | Actor in a leading role | Christian Bale | Vice | NA |\n",
    "| 2019 | Actor in a leading role | Bradley Cooper | A Star Is Born | NA |\n",
    "| 2019 | Actor in a leading role | Willem Dafoe | At Eternity's Gate | NA |\n",
    "| 2019 | Actor in a leading role | Rami Malek | Bohemian Rhapsody | NA |\n",
    "| 2019 | Actor in a leading role | Viggo Mortensen | Green Book | NA |\n",
    "\n",
    "We will begin with writing a parser for a (hopefully!) representative year, then scrape the data for all the years, then apply the scraper to each of those years, and finally combine all the years' data together into a large data set. \n",
    "\n",
    "Let's begin with writing a parser for a (hopefully!) representative year: in this case, 2019 is actually not a great case because it is missing information about who won and lost since (at the time of my writing this notebook) the winners had not been announced. We will use 2018 instead and make the profoundly naÃ¯ve assumption it should work the same going back in time.\n",
    "\n",
    "Start off with using `requests` to get the data and then use `BeautifulSoup` to turn it into soup we can parse through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_raw = requests.get('https://www.oscars.org/oscars/ceremonies/2018').text\n",
    "\n",
    "oscars2018_soup = BeautifulSoup(oscars2018_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Inspect tool exercise from Class 1, the `<div class=\"view-grouping\">` seems to be the most promising tag for us to extract. Use `.find_all('div',{'class':'view-grouping'})` to (hopefully!) get all of these award groups. Inspect the first and last ones to make sure they looks coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the groups that have a <div class=\"view-grouping\"> tag\n",
    "oscars2018_groups = oscars2018_soup.find_all('div',{'class':'view-grouping'})\n",
    "\n",
    "# Inspect the first one\n",
    "oscars2018_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last group is something besides \"Writing (Original Screenplay)\" and it's not clear to me where this tag's content renders on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the last one\n",
    "oscars2018_groups[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This puts us into something of a bind going forward: if the `.find_all` returns more groupings than we expected, then it's not sufficiently precise to identify *only* groupings of nominees.  However, there do not appear to be any child tags in the `oscars2018_groups[0]` grouping that uniquely differentiate them from the child tags present in the `oscars2018_groups[-1]` grouping. Another alternative is to simple parse the first 24 groupings, but this is a very brittle solution since other years' awards might have more or fewer groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oscars2018_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the HTML tree to find more specific parent elements\n",
    "A third alternative is to leverage the tree structure of HTML and get the parent element in the hopes it is more unique than its children. In this case something like `<div id=\"quicktabs-tabpage-honorees-0\"...>` is a promising lead. Use `find_all` to search for this tag and confirm there is only one the one `<div>` element (with its children) rather than multiple `<div>` elements matching \"quicktabs-container-honorees\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new tag group\n",
    "oscars2018_parent_group = oscars2018_soup.find_all('div',{'id':'quicktabs-tabpage-honorees-0'})\n",
    "\n",
    "# Hopefully there is only one group matching this pattern\n",
    "len(oscars2018_parent_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, now we can use `find_all` on the soup for this `<div class=\"view-grouping\">` to search *within* this specific parent group and hopefully there should be the 24 awards groupings. Nope, still 105. This is because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the addition of the [0] since the _parent_group is a list with 1 element in it\n",
    "# We just extract that single element (which is a soup) and then we can use find_all on it\n",
    "\n",
    "oscars2018_true_groups = oscars2018_parent_group[0].find_all('div',{'class':'view-grouping'})\n",
    "\n",
    "len(oscars2018_true_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallelujah! The award names for each group live inside a `<div class=\"view-grouping-header\">`, so we can `find_all` for those, loop through each, and print out the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in oscars2018_parent_group[0].find_all('div',{'class':'view-grouping-header'}):\n",
    "    print(group.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the Oscars site loads a bunch of extra data that it does not render that lives underneath a `<div id=\"quicktabs-tabpage-honorees-1\">` which is where the 81 extra \"awards\" come from. This appears to be an attempt to organize the page by film, rather than by category.\n",
    "\n",
    "### Navigating the HTML tree from a specific child to find specific generic parents\n",
    "Now bear with me through some additional and presently unnecessary pain. Above, we were able to isolate the 24 category groupings we wanted through finding an appropriate *parent* tag and then working *down*. But I also want to show how we could identify the same 24 category groups by finding an appropriate *child* tag and working back up. This could be helpful in other situations where the elements are hard to disambiguate.\n",
    "\n",
    "Let's start by finding the tag for the \"Actor in a Leading Role\" from the soup containing all the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than finding *all* the `<div class=\"view-grouping\">` present in the page, we only want the 23 *siblings* of this specific tag. We can use the `find_next_siblings()` to get these 23 siblings. I do not like this method very much because you have to find the \"eldest\" sibling and then combine it with its siblings later on if you want all the children. In this case, you'd need to keep track of the `<div class=\"view-grouping\">` corresponding to Best Actor and then combine it with its 23 siblings, rather than an approach that simply returns all 24 in a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_group0_next_siblings = oscars2018_groups[0].find_next_siblings()\n",
    "\n",
    "len(oscars2018_group0_next_siblings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also go up to get the parent and then find all 24 of the `<div class='view-grouping'>` among the children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the child we like, get its parent\n",
    "oscars2018_group0_parent = oscars2018_groups[0].parent\n",
    "\n",
    "# Now with the parent, find all the relevant children\n",
    "oscars2018_group0_parent_children = oscars2018_group0_parent.find_all('div',{'class':'view-grouping'})\n",
    "\n",
    "# Confirm\n",
    "len(oscars2018_group0_parent_children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the relevant fields\n",
    "\n",
    "That seemed like a major digression away from the core task of writing a parser, but it is critical that we write a parser that parses *only* the data we want and nothing else. Now that we have our 24 awards groups in `oscars2018_true_groups`, let's break one open and extract all the yummy data waiting inside.\n",
    "\n",
    "There are a few `<div>` sub-classes that are helpfully named that should make extracting this data a bit easier.\n",
    "\n",
    "* `<div class=\"view-grouping-header\">` - name of the category\n",
    "* `<span class=\"golden-text\">` - winner\n",
    "* `<div class=\"views-field views-field-field-actor-name\">` - name of actor\n",
    "* `<div class=\"views-field views-field-title\">` - title of movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Zoom in\" to the `views-field-field-actor-name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0].find_all('div',{'class':\"views-field views-field-field-actor-name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `<h4>` tags may be more specific and helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0].find_all('h4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom into the `views-field-title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0].find_all('div',{'class':\"views-field views-field-title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `<span>` tags may be more specific and helpful, but there are also empty tags here clogging things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0].find_all('span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a battle-scarred web scraper, let me continue to emphasize the importance of quick-checking your assumptions before commiting to writing code. Are these fields still appropriate for other awards categories? Let's check the last category for original screenplay. Are the `<div>`s for \"field-actor-name\" still people and for \"field-title\" still movies? Nope. \n",
    "\n",
    "Looking back at the web page, it's now obvious that the movie title and person who gets the award are flipped between actors/actresses and the other awards categories. We're going to have to keep this in mind going forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[-1].find_all('div',{'class':\"views-field views-field-field-actor-name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[-1].find_all('div',{'class':\"views-field views-field-title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the core parser functionality\n",
    "\n",
    "How will we map the contents of the HTML to the \n",
    "\n",
    "* **Year**: All the awards are from the same year, also in the URL\n",
    "* **Category**: `<h2>`\n",
    "* **Nominee**: `<h4>` for actors, `<span>` for non-actors\n",
    "* **Movie**: `<span>` for actors, `<h4>` for non-actors\n",
    "* **Won**: `<h3>` for sibling, 0 for everyone else; alternatively just the top nominee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2018_true_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = oscars2018_true_groups[0].find_all('h2')[0].text\n",
    "print(\"The name of the category is:\",category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for _nominee in oscars2018_true_groups[0].find_all('h4'):\n",
    "    nominee_name = _nominee.text\n",
    "    names.append(nominee_name)\n",
    "    print(\"The name of a nominee is:\",nominee_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = []\n",
    "\n",
    "for _movie in oscars2018_true_groups[0].find_all('span'):\n",
    "    if len(_movie.text) > 0:\n",
    "        movie_name = _movie.text.strip()\n",
    "        movies.append(movie_name)\n",
    "        print(\"The name of a movie is:\",movie_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy is to use Python's [`zip`](https://docs.python.org/3.7/library/functions.html#zip) library to combine elements from different lists together. But `zip` is a bit too slick and abstract for my tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The elements of each list being combined need to be the same size\n",
    "# So we make a list of the category name and multiply it by 5 to make it the same size as the others\n",
    "list(zip([category]*5,names,movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is to use the `<div class=\"views-row\">`s for each nominee and extract the relevant information from its subdivs. This is a bit more intuitive in the sense of reading from top to bottom and also makes it easier to capture the winner and losers based on position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_nominees = oscars2018_true_groups[0].find_all('div',{'class':'views-row'})\n",
    "\n",
    "for i,nominee in enumerate(actor_nominees):\n",
    "    \n",
    "    # If in the first position, the nominee won\n",
    "    if i == 0:\n",
    "        winner = 'Won'\n",
    "    # Otherwise, the nominee lost\n",
    "    else:\n",
    "        winner = 'Lost'\n",
    "    \n",
    "    # Get a list of all the sub-divs\n",
    "    subdivs = nominee.find_all('div')\n",
    "    \n",
    "    # The first subdiv (for an actor) is the name\n",
    "    name = subdivs[0].text.strip()\n",
    "    \n",
    "    # The second subdiv (for an actor) is the movie name\n",
    "    movie = subdivs[1].text.strip()\n",
    "    \n",
    "    print(\"{0} was nominated for \\\"{1}\\\" and {2}.\".format(name,movie,winner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that reversing \"movie\" and \"name\" works for another award category like original screenplay (`oscars2018_true_groups[-1]`). There's some weirdness with \"Written by\" and \"Story by\" filtering in here rather than simply names that may need to get fixed in the final calculation, but I would want to talk to a domain expert about the differences between these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_screenplay_nominees = oscars2018_true_groups[-1].find_all('div',{'class':'views-row'})\n",
    "\n",
    "for i,nominee in enumerate(original_screenplay_nominees):\n",
    "    if i == 0:\n",
    "        winner = 'Won'\n",
    "    else:\n",
    "        winner = 'Lost'\n",
    "        \n",
    "    subdivs = nominee.find_all('div')\n",
    "    \n",
    "    # movie and name reversed\n",
    "    movie = subdivs[0].text.strip()\n",
    "    name = subdivs[1].text.strip()\n",
    "    \n",
    "    print(\"{0} was nominated for \\\"{1}\\\" and {2}.\".format(name,movie,winner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just for Best Actors, now lets add another layer for all the different awards categories. We can see the movie name and awardee switch is important now since most of the categories are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in oscars2018_true_groups:\n",
    "    category = group.find_all('h2')[0].text\n",
    "    \n",
    "    for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "        if i == 0:\n",
    "            winner = 'Won'\n",
    "        else:\n",
    "            winner = 'Lost'\n",
    "\n",
    "        subdivs = nominee.find_all('div')\n",
    "        name = subdivs[0].text.strip()\n",
    "        movie = subdivs[1].text.strip()\n",
    "\n",
    "        print(\"{0} was nominated in {1} for {2}\\\" and {3}.\".format(name,category,movie,winner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include some flow control, if the name \"actor\" or \"actree\" appears in the category title, then  do nominee name first and movie name second, otherwise do movie name first and nominee name second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in oscars2018_true_groups:\n",
    "    category = group.find_all('h2')[0].text\n",
    "    \n",
    "    if 'Actor' in category or 'Actress' in category:\n",
    "    \n",
    "        for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "            if i == 0:\n",
    "                winner = 'Won'\n",
    "            else:\n",
    "                winner = 'Lost'\n",
    "\n",
    "            subdivs = nominee.find_all('div')\n",
    "            name = subdivs[0].text.strip()\n",
    "            movie = subdivs[1].text.strip()\n",
    "\n",
    "            print(\"{0} was nominated in {1} for \\\"{2}\\\" and {3}.\".format(name,category,movie,winner))\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "            if i == 0:\n",
    "                winner = 'Won'\n",
    "            else:\n",
    "                winner = 'Lost'\n",
    "\n",
    "            subdivs = nominee.find_all('div')\n",
    "            movie = subdivs[0].text.strip()\n",
    "            name = subdivs[1].text.strip()\n",
    "\n",
    "            print(\"\\\"{0}\\\" was nominated in {1} for {2} and {3}.\".format(name,category,movie,winner))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than printing out the information, store it in `nominees_2018` so that we can turn it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominees_2018 = []\n",
    "\n",
    "for group in oscars2018_true_groups:\n",
    "    category = group.find_all('h2')[0].text\n",
    "    \n",
    "    if 'Actor' in category or 'Actress' in category:\n",
    "    \n",
    "        for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "            if i == 0:\n",
    "                winner = 'Won'\n",
    "            else:\n",
    "                winner = 'Lost'\n",
    "\n",
    "            subdivs = nominee.find_all('div')\n",
    "            name = subdivs[0].text.strip()\n",
    "            movie = subdivs[1].text.strip()\n",
    "            \n",
    "            # Swap out the print\n",
    "            # Make a payload for each nominee\n",
    "            nominee_payload = {'Category':category,\n",
    "                               'Name':name,\n",
    "                               'Movie':movie,\n",
    "                               'Year':2018, # We're only looking at 2018 right now\n",
    "                               'Winner':winner}\n",
    "            \n",
    "            # Add the payload to the list of nominees at top\n",
    "            nominees_2018.append(nominee_payload)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "            if i == 0:\n",
    "                winner = 'Won'\n",
    "            else:\n",
    "                winner = 'Lost'\n",
    "\n",
    "            subdivs = nominee.find_all('div')\n",
    "            movie = subdivs[0].text.strip()\n",
    "            name = subdivs[1].text.strip()\n",
    "\n",
    "            # Swap out the print\n",
    "            nominee_payload = {'Category':category,\n",
    "                               'Name':name,\n",
    "                               'Movie':movie,\n",
    "                               'Year':2018,\n",
    "                               'Winner':winner}\n",
    "            \n",
    "            nominees_2018.append(nominee_payload)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominees_df = pd.DataFrame(nominees_2018)\n",
    "nominees_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn this hulking beast of a parser into a function so we can apply it to other years' nominees in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nominees(true_groups,year):\n",
    "    nominees_list = []\n",
    "\n",
    "    for group in true_groups:\n",
    "        category = group.find_all('h2')[0].text\n",
    "\n",
    "        if 'Actor' in category or 'Actress' in category:\n",
    "\n",
    "            for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "                if i == 0:\n",
    "                    winner = 'Won'\n",
    "                else:\n",
    "                    winner = 'Lost'\n",
    "\n",
    "                subdivs = nominee.find_all('div')\n",
    "                name = subdivs[0].text.strip()\n",
    "                movie = subdivs[1].text.strip()\n",
    "                \n",
    "                nominee_payload = {\n",
    "                    'Category':category,\n",
    "                    'Name':name,\n",
    "                    'Movie':movie,\n",
    "                    'Year':year, # We may look at other years\n",
    "                    'Winner':winner\n",
    "                }\n",
    "\n",
    "                nominees_list.append(nominee_payload)\n",
    "\n",
    "        else:\n",
    "\n",
    "            for i,nominee in enumerate(group.find_all('div',{'class':'views-row'})):\n",
    "                if i == 0:\n",
    "                    winner = 'Won'\n",
    "                else:\n",
    "                    winner = 'Lost'\n",
    "\n",
    "                subdivs = nominee.find_all('div')\n",
    "                movie = subdivs[0].text.strip()\n",
    "                name = subdivs[1].text.strip()\n",
    "\n",
    "                nominee_payload = {\n",
    "                    'Category':category,\n",
    "                    'Name':name,\n",
    "                    'Movie':movie,\n",
    "                    'Year':year,\n",
    "                    'Winner':winner\n",
    "                }\n",
    "\n",
    "                nominees_list.append(nominee_payload)\n",
    "    \n",
    "    return nominees_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating vs. parsing to retrieve data\n",
    "\n",
    "Often the data you are interested in is spread across multiple web pages. In an ideal world, the naming conventions would let you retrieve the data from these pages systematically. In the case of the Oscars, the URLs appear to be consistently formatted: `https://www.oscars.org/oscars/ceremonies/2019` suggests that we could change the 2019 to any other date going back to the start of the Oscars and get that year as well: `https://www.oscars.org/oscars/ceremonies/2018` should get us the page for 2018, and so on. Let's demonstrate each of these strategies with the Oscars data: iterating from 2019 back to 1929 in the URL versus parsing the list of links from the header.\n",
    "\n",
    "### Iterating strategies for retrieving data\n",
    "\n",
    "The fundamental assumption with this strategy is that the data are stored at URLs in a consistent way that we can access sequentially. In the case of the Oscars, we *should* be able to simply pass each year to the URL in requests. Here we want to practice responsible data scraping by including a sleep between each request so that we do not overwhelm the Oscars server with requests. We can use the `sleep` function within `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sleep(3)` below prevents any more code from progressing for 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The start of something.\")\n",
    "sleep(3)\n",
    "print(\"The end of something.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core part of the iterating strategy is simply using Python's [`range`](https://docs.python.org/3.7/library/functions.html#func-range) function to generate a sequence of values. Here, we can use `range` to print out a sequence of URLs that should correspond to awards pages from 2010 through 2019. We can also incorporate the `sleep` functionality and wait a second between each `print` statementâ€”it should now take 10 seconds for this code to finish printing. This simulates how we can use `sleep` to slow down and spread out requests so that we do not overwhelm the servers whose data we are trying to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2010,2020):\n",
    "    sleep(1)\n",
    "    print('https://www.oscars.org/oscars/ceremonies/{0}'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a function `parse_nominees` above that takes the \"true groups\" of nominees. Let's try to tie these pieces together for all the nominees in the 2010s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the data we get\n",
    "all_years_nominees = dict()\n",
    "\n",
    "# For each year starting in 2010 until 2019\n",
    "for year in range(2010,2020):\n",
    "    \n",
    "    # Pause for a second between each request\n",
    "    sleep(1)\n",
    "    \n",
    "    # Get the raw HTML\n",
    "    year_raw_html = requests.get('https://www.oscars.org/oscars/ceremonies/{0}'.format(year)).text\n",
    "\n",
    "    # Soup-ify\n",
    "    year_souped_html = BeautifulSoup(year_raw_html)\n",
    "    \n",
    "    # Get the parent group\n",
    "    year_parent_group = year_souped_html.find_all('div',{'id':'quicktabs-tabpage-honorees-0'})\n",
    "    \n",
    "    # Get the true groups under the parent group\n",
    "    year_true_groups = year_parent_group[0].find_all('div',{'class':'view-grouping'})\n",
    "    \n",
    "    # Use our parsing function, passing the year from above\n",
    "    year_nominees = parse_nominees(year_true_groups,year)\n",
    "    \n",
    "    # Convert the year_nominees to a DataFrame and add them to all_years_nominees\n",
    "    all_years_nominees[year] = pd.DataFrame(year_nominees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine each of the DataFrames in `all_years_nominees` into a giant DataFrame of all the nominees from 2010-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_nominees_df = pd.concat(all_years_nominees)\n",
    "all_years_nominees_df.reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing strategy for retrieving data\n",
    "\n",
    "Frustratingly, this iterating strategy may not always hold: maybe some years are skipped or the naming convention changes at some point. We will cover some basics of [error-handling in Python](https://realpython.com/python-exceptions/) that could let us work around errors as they pop up, but this may result in an incomplete collection if the naming conventions are systematic. What we would want to do is to identify all the links ahead of time by parsing them from list and then work through that list to get the complete data collection.\n",
    "\n",
    "What this means in the context of our Oscars example is assuming that we cannot trust that the sequential numbering of the years is a realiable guide to get all the data. Instead, we should get a list of the URLs for each of the awards pages from the \"ceremonies-decade-scroller\" (from Inspect) at the top. This scroller *should* be consistent across all the pages, but start with the nominees for 2019 just to be safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2019_raw = requests.get('https://www.oscars.org/oscars/ceremonies/2019').text\n",
    "oscars2019_soup = BeautifulSoup(oscars2019_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Inspect tool, there is a `<div class=\"years\">` that contains the links to each of the years. Run a `.find_all` to get all these href locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the <div class=\"years\"> as a parent tag first, just in case there are <a class=\"years\"> elsewhere\n",
    "oscars2019_years_div = oscars2019_soup.find_all('div',{'class':'years'})[0]\n",
    "\n",
    "# Now get the <a class=\"years\"> underneath only the oscars2019_years_div\n",
    "oscars2019_years_a = oscars2019_years_div.find_all('a',{'class':'year'})\n",
    "\n",
    "# Inspect the first 10\n",
    "oscars2019_years_a[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these `<a>` tags contains an \"href\", or the URL element where the page lives, and a text element for what's displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2019_years_a[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars2019_years_a[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a loop to print out the URL locations for all the other award years based on the \"official\" links in the \"ceremonies-decade-scroller\" navigation rather than assuming the years are sequentialâ€”I promise this will pay dividends in the future when inconsistent design wreaks havoc on your sequential data strategies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in oscars2019_years_a[-10:]:\n",
    "    href = a['href']\n",
    "    print('https://www.oscars.org' + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `parse_nominees` function for these pages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the data we get\n",
    "all_years_nominees = dict()\n",
    "\n",
    "# For the 10 most recent years\n",
    "for a in oscars2019_years_a[-10:]:\n",
    "    \n",
    "    # Pause for a second between each request\n",
    "    sleep(1)\n",
    "    \n",
    "    # Get the href\n",
    "    href = a['href']\n",
    "    \n",
    "    # Get the year\n",
    "    year = a.text\n",
    "    \n",
    "    # Get the raw HTML\n",
    "    url = 'https://www.oscars.org' + href\n",
    "    year_raw_html = requests.get(url).text\n",
    "\n",
    "    # Soup-ify\n",
    "    year_souped_html = BeautifulSoup(year_raw_html)\n",
    "    \n",
    "    # Get the parent group\n",
    "    year_parent_group = year_souped_html.find_all('div',{'id':'quicktabs-tabpage-honorees-0'})\n",
    "    \n",
    "    # Get the true groups under the parent group\n",
    "    year_true_groups = year_parent_group[0].find_all('div',{'class':'view-grouping'})\n",
    "    \n",
    "    # Use our parsing function, passing the year from above\n",
    "    year_nominees = parse_nominees(year_true_groups,year)\n",
    "    \n",
    "    # Convert the year_nominees to a DataFrame and add them to all_years_nominees\n",
    "    all_years_nominees[year] = pd.DataFrame(year_nominees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine each of the DataFrames in `all_years_nominees` into a giant DataFrame of all the nominees from 2010-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_nominees_df = pd.concat(all_years_nominees)\n",
    "all_years_nominees_df.reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project time\n",
    "\n",
    "Let's take a look at websites you're interested in scraping and see what kinds of challenges and opportunities exist for scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
