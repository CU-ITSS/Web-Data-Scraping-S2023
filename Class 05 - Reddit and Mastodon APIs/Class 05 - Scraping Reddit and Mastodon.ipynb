{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2023 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping the Internet Archive and Wikipedia APIs\n",
    "* **Week 5**: Scraping the Reddit and Mastodon APIs\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "This notebook is adapted from excellent notebooks in Dr. [Cody Buntain](http://cody.bunta.in/)'s seminar on [Social Media and Crisis Informatics](http://cody.bunta.in/teaching/2018_winter_umd_inst728e/) as well as the [PRAW documentation](https://praw.readthedocs.io/en/latest/).\n",
    "\n",
    "## Class 5 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Authenticating with a closed API \n",
    "* Retrieving data from the Reddit API using [PRAW](https://praw.readthedocs.io/en/stable/) and [PSAW](https://github.com/dmarx/psaw)\n",
    "* Retrieving data from the Mastodon API using [`mastodon.py`](https://mastodonpy.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a few common libraries for all these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to other servers on the web\n",
    "import requests\n",
    "\n",
    "# APIs spit out data in JSON\n",
    "import json\n",
    "\n",
    "# Use BeautifulSoup to parse some HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Handling dates and times\n",
    "from datetime import datetime\n",
    "\n",
    "# DataFrames!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit\n",
    "\n",
    "Reddit also hosts a lot of detailed behavioral data that could be of interest to social scientists. As was the case with Wikipedia, our naïve inclination may be to develop scrapers and parsers to extract this information, but Reddit will give much of it to you for free through their API!\n",
    "\n",
    "You can retrieve a few different types of entities from Reddit's API: sub-reddits, submissions, comments, and redditors. Many of these are interoperable: a sub-reddit contains submissions contributed by redditors with comments from other redditors.\n",
    "\n",
    "We will use a wrapper library to communicate with the Reddit API called [Python Reddit API Wrapper](https://praw.readthedocs.io/en/latest/) or `praw`. \n",
    "\n",
    "Copy the code below to your terminal to install `praw`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! conda install -c conda-forge praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can import `praw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to authenticate with Reddit to get access to the API. Typically you can just enter the client ID, client secret, password, username, *etc*. as strings. \n",
    "\n",
    "1. You will need to create an account on Reddit. After you have created an account and logged in, go to https://www.reddit.com/prefs/apps/. \n",
    "2. Scroll down and click the \"create app\" button at the bottom. Provide a basic name, description, and enter a URL for your homepage (or just use http://www.colorado.edu).\n",
    "3. You will need the client ID (the string of characters beneath the name of your app) as well as the secret (the other string of characters) as well as your username and password.\n",
    "4. You can make up a user-agent string, but include your username as good practice for the sysadmins to track you down if you break things.\n",
    "\n",
    "![Image from Cody Buntain](http://www.cs.umd.edu/~cbuntain/inst728e/reddit_screens/1-003a.png)\n",
    "\n",
    "You'll create an API connector object (`r`) below that will authenticate with the API and handle making the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = praw.Reddit(\n",
    "    client_id='your application id',\n",
    "    client_secret='your application secret',\n",
    "    password='your account password',\n",
    "    user_agent='scraping script by /u/youraccountname',\n",
    "    username='your account name'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that this authentication process worked by making a simple request like printing your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to read them in from a local file (\"login.json\") so that I post this notebook on the internet in the future without compromising my account security. This won't work for you, so just skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load my credentials from a local disk so I don't show the world\n",
    "with open('reddit_login.json','r') as f:\n",
    "    r_creds = json.load(f)\n",
    "    \n",
    "# Create an authenticated reddit instance using the creds\n",
    "r = praw.Reddit(client_id = r_creds['client_id'],\n",
    "                client_secret = r_creds['client_secret'],\n",
    "                password = r_creds['password'],\n",
    "                user_agent = r_creds['user_agent'],\n",
    "                username = r_creds['username'])\n",
    "\n",
    "# Make sure your reddit instance works\n",
    "print(r.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-reddits\n",
    "Now print the top 25 stories in /r/news.\n",
    "\n",
    "[Documentation for the Subreddit model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `news_subreddit` object to store the various attributes about this sub-reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit = r.subreddit('news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `news_subreddit` has a number of attributes and methods you can call on it. The time the sub-reddit was founded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.created_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's formatted in a UNIX timecode (seconds since 1 January 1970), but we can convert it into a more readable timestamp with `datetime`'s `utcfromtimestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.utcfromtimestamp(news_subreddit.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other attributes such as the number of subscribers, current active users, as well as the description of the sub-reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{0:,}'.format(news_subreddit.subscribers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.over18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.active_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of the sub-reddit are available as a method `.rules()` which returns a list of dictionaries of rule objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_subreddit.rules()['rules']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When were each of these rules created? Loop through each of the rules and print the \"short_name\" of the rule and the rule timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in news_subreddit.rules()['rules']:\n",
    "    created = rule['created_utc']\n",
    "    print(rule['short_name'], datetime.utcfromtimestamp(created))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a list of the moderators for this subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = []\n",
    "\n",
    "for mod in news_subreddit.moderator():\n",
    "    mod_list.append(mod.name)\n",
    "    \n",
    "mod_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "We can get a list of submissions to a sub-reddit using [a few different methods](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html).\n",
    "\n",
    "* `.controversial()`\n",
    "* `.hot()`\n",
    "* `.new()`\n",
    "* `.rising()`\n",
    "* `.search()`\n",
    "* `.top()`\n",
    "\n",
    "Here we will use the `.top()` method to get the top 25 submissions on the /r/news subreddit from the past 12 months.\n",
    "\n",
    "[Documentation for the Submission model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_news = r.subreddit('news').top('year',limit=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top25_news` is a `ListingGenerator` object, which is a special [generator](https://www.dataquest.io/blog/python-generators-tutorial/) class defined by PRAW. It does not actually go out and get the data at this stage. There's not much you can do to look inside this `ListingGenerator` other than loop through and perform operations. In this case, lets add each submission to a list of `top25_submissions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_submissions = []\n",
    "\n",
    "for submission in r.subreddit('news').top('year',limit=25):\n",
    "    top25_submissions.append(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first (top) `Submission` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_submission = top25_submissions[0]\n",
    "first_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `dir` function to see the other methods and attributes inside this first top `Submission` object. (There are a lot of other \"hidden\" attributes and methods that use the \"\\_\" which we can ignore with this list comprehension.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in dir(first_submission) if '_' not in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vars` may be even more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(first_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the features of each submission, store them in a dictionary, and save to an external list. This step will take a while (approximately one second per submission) because we make an API call for each submission in the `ListingGenerator` returned by the `r.subreddit('news').top('year',limit=25)` we're looping through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_stats = []\n",
    "\n",
    "for submission in r.subreddit('news').top('year',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = submission.id\n",
    "    d['title'] = submission.title\n",
    "    d['num_comments'] = submission.num_comments\n",
    "    d['score'] = submission.score\n",
    "    d['upvote_ratio'] = submission.upvote_ratio\n",
    "    d['date'] = datetime.utcfromtimestamp(submission.created_utc)\n",
    "    d['domain'] = submission.domain\n",
    "    d['gilded'] = submission.gilded\n",
    "    d['num_crossposts'] = submission.num_crossposts\n",
    "    d['nsfw'] = submission.over_18\n",
    "    if submission.author is not None:\n",
    "        d['author'] = submission.author.name\n",
    "    submission_stats.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn `submission_stats` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_df = pd.DataFrame(submission_stats)\n",
    "top25_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the relationship between score and number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top25_df.plot.scatter(x='score',y='num_comments',s=50,c='k',alpha=.5)\n",
    "# ax.set_xlim((0,200000))\n",
    "# ax.set_ylim((0,16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "This is a simple Reddit submission: [What is a dataset that you can't believe is available to the public?](https://www.reddit.com/r/datasets/comments/akb4mr/what_is_a_dataset_that_you_cant_believe_is/). We can inspect the comments in this simple submission.\n",
    "\n",
    "[Documentation for Comment model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/comment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe = r.submission(id='akb4mr')\n",
    "\n",
    "print(\"This submission was made on {0}.\".format(datetime.utcfromtimestamp(cant_believe.created_utc)))\n",
    "print(\"There are {0:,} comments.\".format(cant_believe.num_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect these comments, working from the [Comment Extraction and Parsing](https://praw.readthedocs.io/en/latest/tutorials/comments.html) tutorial in PRAW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe.comments.replace_more(limit=None)\n",
    "\n",
    "for comment in cant_believe.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each comment has a lot of metadata we can preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe_comment_metadata = []\n",
    "\n",
    "for comment in cant_believe.comments.list():\n",
    "    if not comment.collapsed: # Skip collapsed/deleted comments\n",
    "        d = {}\n",
    "        d['id'] = comment.id\n",
    "        d['parent_id'] = comment.parent_id\n",
    "        d['body'] = comment.body\n",
    "        d['depth'] = comment.depth\n",
    "        d['edited'] = comment.edited\n",
    "        d['score'] = comment.score\n",
    "        d['date'] = datetime.utcfromtimestamp(comment.created_utc)\n",
    "        d['submission_id'] = comment.submission.id\n",
    "        d['submission_title'] = comment.submission.title\n",
    "        d['subreddit'] = comment.subreddit.display_name\n",
    "        if comment.author is not None:\n",
    "            d['author'] = comment.author.name\n",
    "        cant_believe_comment_metadata.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_believe_df = pd.DataFrame(cant_believe_comment_metadata)\n",
    "\n",
    "# How long is the comment\n",
    "cant_believe_df['comment_length'] = cant_believe_df['body'].str.len()\n",
    "\n",
    "cant_believe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do comments deeper in this comment tree have lower scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x='depth',y='score',data=cant_believe_df,kind='bar',color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do comments deeper in this comment tree have shorter lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x='depth',y='comment_length',data=cant_believe_df,kind='bar',color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redditors\n",
    "\n",
    "A Redditor is a user and we can get meta-data about the account as well as the history of the user's comments and submissions from the API.\n",
    "\n",
    "[Documentation for the Redditor model in PRAW](https://praw.readthedocs.io/en/latest/code_overview/models/redditor.html).\n",
    "\n",
    "How much link and comment karma does this user have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez = r.redditor('spez')\n",
    "print(\"Link karma: {0:,}\".format(spez.link_karma))\n",
    "print(\"Comment karma: {0:,}\".format(spez.comment_karma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Reddit flags the users who are employees of Reddit as well as if accounts have verified email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez.is_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez.has_verified_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the time this user's account was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(spez.created_utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get information about individual redditors' submissions and comment histories. Here we will use u/spez (the CEO of Reddit), get his top-voted submissions, and loop through them to get the data for each submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez_submissions = []\n",
    "\n",
    "for submission in r.redditor('spez').submissions.top('all',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = submission.id\n",
    "    d['title'] = submission.title\n",
    "    d['num_comments'] = submission.num_comments\n",
    "    d['score'] = submission.score\n",
    "    d['upvote_ratio'] = submission.upvote_ratio\n",
    "    d['date'] = datetime.utcfromtimestamp(submission.created_utc)\n",
    "    d['domain'] = submission.domain\n",
    "    d['gilded'] = submission.gilded\n",
    "    d['num_crossposts'] = submission.num_crossposts\n",
    "    d['nsfw'] = submission.over_18\n",
    "    if comment.author is not None:\n",
    "        d['author'] = submission.author.name\n",
    "    spez_submissions.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can turn this list of dictionaries into a DataFrame to do substantive data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_submissions).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get all the comments made by an editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spez_comments = []\n",
    "\n",
    "for comment in r.redditor('spez').comments.top('all',limit=25):\n",
    "    d = {}\n",
    "    d['id'] = comment.id\n",
    "    d['body'] = comment.body\n",
    "    try:\n",
    "        d['depth'] = comment.depth\n",
    "    except:\n",
    "        d['depth'] = np.nan\n",
    "    d['edited'] = comment.edited\n",
    "    d['score'] = comment.score\n",
    "    d['date'] = datetime.utcfromtimestamp(comment.created_utc)\n",
    "    d['submission_id'] = comment.submission.id\n",
    "    d['submission_title'] = comment.submission.title\n",
    "    d['subreddit'] = comment.subreddit.display_name\n",
    "    if comment.author is not None:\n",
    "        d['author'] = comment.author.name\n",
    "    spez_comments.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_comments).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This user's top comments are mostly focused in the /r/announcements subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(spez_comments)['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archived data via PushShift\n",
    "\n",
    "PushShift is a researcher-maintained archive of Reddit posts and comments. Full data dumps of [submissions](https://files.pushshift.io/reddit/submissions/) and [comments](https://files.pushshift.io/reddit/comments/) are available, although these are (unsurprisingly) very space intensive. You can also access an API to make [ElasticSearch](https://www.elastic.co/elasticsearch/) queries against a database of this archive of submissions and comments. Unfortunately, the service is frequently down. \n",
    "\n",
    "We will use the [`pmaw`](https://github.com/mattpodolak/pmaw) library to access this data endpoint using Python. Install `pmaw` once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pmaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the `PushshiftAPI` class from `psaw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the submission history for a subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = api.search_submissions(subreddit=\"wallstreetbets\",limit=10000,score=\">1000\")\n",
    "\n",
    "submissions_list = [p for p in submissions]\n",
    "\n",
    "len(submissions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Mastodon\n",
    "\n",
    "After Twitter's acquisition by Elon Musk in October 2022, the service rapidly deteriorated through neglect and mismanagement. Given the increasingly precarity of Twitter's API access, I do not recommend that researchers build projects around Twitter data access and availability. \n",
    "\n",
    "I am consciously moving this course towards alternatives like [Mastodon](https://joinmastodon.org/). Mastodon and its API have similar but not identical affordances as Twitter as both a user and developer. **Importantly**, Mastodon users are much more resistant to having their data being collected, even by researchers. Users can [opt-out of being indexed](https://docs.joinmastodon.org/user/preferences/#misc) by search engines and the \"noindex\" meta-data value included in their user information.\n",
    "\n",
    "Like most semi-public APIs, you will need to register an account first. I would recommend the following academic-focused instances, and there are [many others](https://github.com/nathanlesage/academics-on-mastodon):\n",
    "* [sciences.social](https://sciences.social/) - General social sciences\n",
    "* [hcommons.social](https://hcommons.social/) - Digital humanities\n",
    "* [mstdn.science](https://mstdn.science/) - Microbiology initially\n",
    "* [datasci.social](https://datasci.social/) - Data science\n",
    "* [historians.social](https://historians.social/) - Historians\n",
    "* [hci.social](https://hci.social/) - Human-computer interaction\n",
    "* [sigmoid.social](https://sigmoid.social/) - AI\n",
    "\n",
    "We will use the [Mastodon.py](https://mastodonpy.readthedocs.io/) library. Install from pip once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Mastodon.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Mastodon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mastodon import Mastodon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an application using your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mastodon.create_app(\n",
    "    client_name = 'itss',\n",
    "    scopes = ['read'],\n",
    "    api_base_url = 'https://hci.social',\n",
    "    to_file = 'itss_credentials.secret'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up *my* credentials for logging into my account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('login.json','r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masto = Mastodon(client_id = 'itss_credentials.secret')\n",
    "masto.log_in(\n",
    "    username = credentials['email'],\n",
    "    password = credentials['password'],\n",
    "    scopes = ['read'],\n",
    "    to_file = 'itss_user_credentials.secret'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your app is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masto.app_verify_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about my account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masto.me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for another account. [Eugen Rochko](https://mastodon.social/@Gargron) (@Gargron) is the lead developer of Mastodon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masto.account_search(\"Gargron@mastodon.social\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some recent statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bkeegan_statuses = masto.account_statuses(108280556926327188,limit=500)\n",
    "bkeegan_statuses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get who I follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkeegan_following = masto.account_following(108280556926327188)\n",
    "bkeegan_following[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get who follows me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkeegan_followers = masto.account_followers(108280556926327188)\n",
    "bkeegan_followers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get my local timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_timeline = masto.timeline_local()\n",
    "local_timeline[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for toots with a given hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_toots = masto.timeline_hashtag('theorizingthefediverse')\n",
    "hashtag_toots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
